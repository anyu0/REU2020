\documentclass[12pt]{article}
\usepackage{ragged2e}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm, hhline, enumitem}
\usepackage{stmaryrd, mathtools}
\usepackage{amsmath, amssymb, graphicx}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\DeclareMathOperator{\ex}{\mathbb{E}}
\DeclareMathOperator{\pr}{\mathbb{P}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\sgn}{sgn}
\begin{document}
	\begin{flushright}
		Summer 2020
	\end{flushright}
	
	\begin{center}
		\LARGE\textbf{REU Practice Problems}
	\end{center}


\section{Topology and measurability}

	We let $\Sigma$ denote a set $\llbracket p, q\rrbracket = \{p,p+1,\dots,q-1,q\}$ for $p\in\mathbb{N}$, $q\in\mathbb{N}\cup\{\infty\}$, and let $\Lambda$ denote an interval in $\mathbb{R}$ with endpoints $a\leq b$. We write $C(X)$ for the space of continuous real-valued functions on $X$ with the topology of compact convergence and the Borel $\sigma$-algebra $\mathcal{C}$. Recall that this topology is generated by the basis of sets
	\[
	B_K(f,\epsilon) := \big\{g\in C(X):\sup_{x\in K} |f(x)-g(x)|<\epsilon\big\},
	\]
	with $K\subset X$ is compact, $f\in C(X)$, and $\epsilon>0$. When $X=\Sigma\times\Lambda$, we write $(C(\Sigma\times\Lambda),\mathcal{C}_\Sigma)$.

	\subsection*{Problem 1}
	
		We aim to construct a metric $d:C(\Sigma\times\Lambda)\times C(\Sigma\times\Lambda)\to [0,\infty)$ which induces the topology of compact convergence on $C(\Sigma\times\Lambda)$. The idea is to obtain a compact exhaustion of $\Sigma\times\Lambda$, i.e., a countable collection of compact sets $K_n\subset\Sigma\times\Lambda$ such that $\bigcup_n K_n = \Sigma\times\Lambda$, and such that every compact subset of $\Sigma\times\Lambda$ is contained in some $K_n$. We then construct $d$ from the sup-metrics on each of these sets $K_n$. We define the sets
		\[
		K_n := \Sigma_n \times \Lambda_n = \llbracket p, q_n\rrbracket \times [a_n,b_n]
		\]
		as follows. We let $q_n = \min(p+n,q)$. If $a\in\Lambda$, i.e, $\Lambda$ is closed at the left, then $a_n=a$ for all $n$, and likewise $b_n=b$ if $b\in\Lambda$. If $a\notin\Lambda$, we let $a_n\in\mathbb{R}$, $a_n>a$ be a sequence decreasing to $a$, for instance $a_n=a+\frac{1}{n}$ if $a>-\infty$, or $a_n=-n$ if $a_n=-\infty$. If $b\notin\Lambda$, we let $b_n\nearrow b$. In any case, we see that the sets $K_1\subset K_2\subset\cdots\subset\Sigma\times\Lambda$ are compact, they cover $\Sigma\times\Lambda$, and any compact subset $K$ of $\Sigma\times\Lambda$ is contained in all $K_n$ for sufficiently large $n$.
		
		We now define, for each $n$ and $f,g\in C(\Sigma\times\Lambda)$,
		\[
		d_n(f,g) := \sup_{(i,t)\in K_n} |f(i,t)-g(i,t)|,\quad d_n'(f,g) := \min\{d_n(f,g), 1\} 
		\]
		Clearly each $d_n$ is nonnegative and satisfies the triangle inequality, and it is then easy to see that the same properties hold for $d_n'$. Furthermore, $d_n'\leq 1$, so we can define
		\[
		d(f,g) := \sum_{n=1}^\infty 2^{-n} d_n'(f,g).
		\]
		We first observe that $d$ is a metric on $C(\Sigma\times\Lambda)$. Indeed, it is nonnegative, and if $f=g$, then each $d_n'(f,g)=0$, so the sum is 0. Conversely, if $f\neq g$, then since the $K_n$ cover $\Sigma\times\Lambda$, we can choose $n$ large enough so that $K_n$ contains an $x$ with $f(x)\neq g(x)$. Then $d_n'(f,g)\neq 0$, and hence $d(f,g)\neq 0$. The triangle inequality holds for $d$ since it holds for each $d_n'$.
		
		Now we prove that the topology $\tau_d$ on $C(\Sigma\times\Lambda)$ induced by $d$ is the same as the topology of compact convergence, which we will denote $\tau_c$. First, choose $\epsilon>0$ and $f\in C(\Sigma\times\Lambda)$. Let $g\in B^d_\epsilon(f)$, i.e., $d(f,g)<\epsilon$. We will find a set $A_g\in\tau_c$ such that $g\in A_g\subset B^d_\epsilon(f)$. Let $\delta := d(f,g)$, and choose $n$ large enough so that $\sum_{k>n} 2^{-k} < \frac{\epsilon-\delta}{2}$. Define $A_g := B_{K_n}(g,\frac{\epsilon-\delta}{n})$, and suppose $h\in A_g$. Then since $K_m\subseteq K_n$ for $m\leq n$, we have
		\begin{align*}
		d(f,h) &\leq d(f,g) + d(g,h)\\
		&\leq \delta + \sum_{k=1}^n 2^{-k}d_n(g,h) + \sum_{k>n} 2^{-k}\\
		&\leq \delta + \frac{\epsilon-\delta}{2} + \frac{\epsilon-\delta}{2} = \epsilon.
		\end{align*}
		Therefore $g\in A_g\subset B^d_\epsilon(f)$. It follows that $B^d_\epsilon(f)\in \tau_c$. Indeed, we can write
		\[
		B^d_\epsilon(f) = \bigcup_{g\in B^d_\epsilon(f)} A_g,
		\]
		a union of elements of $\tau_c$. This proves that $\tau_d\subseteq\tau_c$.
		
		To prove the converse, let $K\subset\Sigma\times\Lambda$ be compact, $f\in C(\Sigma\times\Lambda)$, and $\epsilon>0$. Choose $n$ so that $K\subset K_n$, and let $g\in B_K(f,\epsilon)$ and $\delta:= \sup_{x\in K} |f(x)-g(x)|$. If $d(g,h) < 2^{-n}(\epsilon-\delta)$, then $d_n'(g,h) \leq 2^n d(g,h) < \epsilon-\delta$, hence $d_n(g,h) < \epsilon-\delta$. It follows that
		\begin{align*}
		\sup_{x\in K} |f(x)-h(x)| &\leq \delta + \sup_{x\in K} |g(x)-h(x)| \leq \delta + d_n(g,h)\\
		&\leq \delta + \epsilon-\delta = \epsilon.
		\end{align*}
		Thus $g\in B^d_{2^{-n}(\epsilon-\delta)}(f) \subset B_K(f,\epsilon)$. It follows that $\tau_c\subseteq \tau_d$, and we conclude that $\tau_d = \tau_c$.
		
		Next, we show that $(C(\Sigma\times\Lambda), d)$ is a complete metric space. Let $(f_n)_{n\geq 1}$ be Cauchy with respect to $d$. Then we claim that $(f_n)$ must be Cauchy with respect to $d_n'$, on each $K_n$. Indeed, $d(f_\ell, f_m) \geq 2^{-n}d_n'(f_\ell, f_m)$, so if $(f_n)$ were not Cauchy with respect to $d_n'$, it would not be Cauchy with respect to $d$ either. Thus $(f_n)$ is uniformly Cauchy on each $K_n$, and hence converges uniformly to a limit $f^{K_n}$ on each $K_n$. Since the limit must be unique at each point of $\Sigma\times\Lambda$, we have $f^{K_n}(x) = f^{K_m}(x)$ if $x\in K_n\cap K_m$. Since $\bigcup K_n = \Sigma\times\Lambda$, we obtain a well-defined function $f$ on all of $\Sigma\times\Lambda$ given by $f(x)=f^{K_n}(x)$, where $x\in K_n$. Given any compact $K\subset \Sigma\times\Lambda$, if $n$ is large enough so that $K\subset K_n$, then because $f_n \to f^{K_n} = f|_{K_n}$ uniformly on $K_n$, we have $f_n \to f^{K_n}|_K = f|_K$ uniformly on $K$. That is, for any $K\subset\Sigma\times\Lambda$ compact and $\epsilon>0$, we have $f_n \in B_K(f,\epsilon)$ for all sufficiently large $n$. Therefore $(f_n)$ converges to $f$ in the topology of compact convergence, and equivalently in the metric $d$.
		
		Lastly, we prove separability, c.f. example 1.3 in Billingsley, \textit{Convergence of Probability Measures}. For each pair of positive integers $n,k$, let $D_{n,k}$ be the subcollection of $C(\Sigma\times\Lambda)$ consisting of polygonal functions that are piecewise linear on $\{j\}\times I_{n,k,i}$ for each $j\in\Sigma_n$ and each subinterval 
		\[
		I_{n,k,i} := [a_n+\tfrac{i-1}{k}(b_n-a_n), a_n+\tfrac{i}{k}(b_n-a_n)], \quad 1\leq i\leq k,
		\] 
		taking rational values at the endpoints of these subintervals, and extended linearly to all of $\Lambda = [a,b]$. Then $D := \bigcup_{n,k} D_{n,k}$ is countable, and we claim that it is dense in the topology of compact convergence. To see this, let $K\subset\Sigma\times\Lambda$ be compact, $f\in C(\Sigma\times\Lambda)$, and $\epsilon>0$, and choose $n$ so that $K\subset K_n$. Since $f$ is uniformly continuous on $K_n$, we can choose $k$ large enough so that for $0\leq i\leq k$, if $t\in I_{n,k,i}$, then $|f(j,t) - f(j, a_n + \frac{i}{k}(b_n-a_n))| < \epsilon/2$ for all $j\in\Sigma_n$. We then choose $g\in \bigcup_k D_{n,k}$ with $|g(j,a_n + \frac{i}{k}(b_n-a_n)) - f(j,a_n + \frac{i}{k}(b_n-a_n))| < \epsilon/2$. Then $f(j,t)$ is within $\epsilon$ of both $g(j,a_n + \frac{i-1}{k}(b_n-a_n))$ and $g(j,a_n + \frac{i}{k}(b_n-a_n))$. Since $g(j,t)$ lies between these two values, $f(j,t)$ is with $\epsilon$ of $g(j,t)$ as well. In summary,
		\[
		\sup_{(j,t)\in K} |f(j,t)-g(j,t)| \leq \sup_{(j,t)\in K_n} |f(j,t)-g(j,t)| < \epsilon,
		\] 
		so $g\in B_K(f,\epsilon)$. This proves that $D$ is a countable dense subset of $C(\Sigma\times\Lambda)$. We conclude that $(C(\Sigma\times\Lambda),\tau_c)$ is a Polish space.
		
		
	\section*{Problem 2}
	
		Let $(\Omega,\mathcal{F},\pr)$ be a probability space and $X,Y$ random variables on $(\Omega,\mathcal{F},\pr)$ taking values in $C(\Sigma\times\Lambda)$, where $\Sigma = \llbracket 1, N\rrbracket$ with $N\in\mathbb{N}$ or $N=\infty$. We consider the collection $\mathcal{S}_X$ of sets of the form
		\[
		\{\omega\in\Omega : X(\omega)(i_1,t_1)\leq x_1,\dots,X(\omega)(i_n,t_n)\leq x_n\} = \bigcap_{k=1}^n X(i_k,t_k)^{-1}(-\infty,x_k],
		\] 
		ranging over all $n\in\mathbb{N}$, $(i_1,t_1),\dots,(i_n,t_n)\in \Sigma\times\Lambda$, and $x_1,\dots,x_n\in\mathbb{R}$. We first prove that $\mathcal{S}_X \subset \mathcal{F}$. We can write 
		\[
		\{X(i_k,t_k)\leq x_k\} = X^{-1}(\{f\in C(\Sigma\times\Lambda):f(i_k,t_k)\leq x_k\}).
		\]
		We claim that the set $\{f\in C(\Sigma\times\Lambda):f(i_k,t_k)\leq x_k\}$ is closed in the topology of compact convergence. If $f_n(i_k,t_k)\leq x_k$ for all $n$ and $f_n\to f$ in the topology of compact convergence, then by taking limits on a compact set containing $(i_k,t_k)$, we find $f(i_k,t_k)\leq x_k$ as well. This proves the claim, and it follows from the measurability of $X$ that $\{X(i_k,t_k)\leq x_k\} = X^{-1}(\{f(i_k,t_k)\leq x_k\})\in\mathcal{F}$. The finite intersection is thus also in $\mathcal{F}$, proving that $\mathcal{S}_X \subset \mathcal{F}$. On the other hand, it is clear that $\{\omega\in\Omega:X(\omega)\in A\} = X^{-1}(A)\in\mathcal{F}$ for any $A\in\mathcal{C}_\Sigma$ since $X$ is measurable.
		
		Now we prove that $\mathbb{P}|_{\mathcal{S}_X}$ determines the distribution $\mathbb{P}\circ X^{-1}$. To do so, note that $\mathcal{S}_X = \sigma(\{X^{-1}(A) : A\in\mathcal{S}\})$, where $\mathcal{S}$ is the collection of cylinder sets
		\[
		\{f\in C(\Sigma\times\Lambda) : f(i_1,t_1)\in A_1, \dots, f(i_n,t_n) \in A_n\}, \quad A_1,\dots,A_n\in\mathcal{B}(\mathbb{R}). 
		\]
		This follows from the fact that $\mathcal{B}(\mathbb{R})$ is generated by intervals of the form $(-\infty,x]$. Furthermore, this fact, along with the fact proven above that $\{f(i_k,t_k)\in (-\infty,x_k]\}$ is closed, show that $\mathcal{S}\subset\mathcal{C}_\Sigma$. Observe that the intersection of two elements of $\mathcal{S}$ is clearly another element of $\mathcal{S}$, so $\mathcal{S}$ is a $\pi$-system. We now argue that $\mathcal{S}$ generates the Borel sets, i.e., $\sigma(\mathcal{S}) = \mathcal{C}_\Sigma$. Since $\mathcal{S}\subset \mathcal{C}_\Sigma$, we have $\sigma(\mathcal{S})\subseteq \mathcal{C}_\Sigma$. To prove the opposite inclusion, let $K\subset\Sigma\times\Lambda$ be compact, $f\in C(\Sigma\times\Lambda)$, and $\epsilon>0$, and let $H$ be a countable dense subset of $K$. (Recall that every compact metric space is separable, and $K$ is homeomorphic to a product of finitely many compact sets in $\mathbb{R}$, which are metrizable. So $K$ is separable.) We claim that
		\[
		B_K(f,\epsilon) = \bigcup_{n=1}^\infty\,\bigcap_{(i,t)\in H} \{g\in C(\Sigma\times\Lambda) : g(i,t) \in  (f(i,t)-(1-2^{-n})\epsilon, f(i,t) + (1-2^{n})\epsilon)\}.
		\]
		Indeed, if $g\in B_K(f,\epsilon)$, i.e., $\sup_{(i,t)\in K} |g(i,t)-f(i,t)| < \epsilon$. Then since $1-2^{-m}\nearrow 1$, we can choose $m$ large enough so that 
		\[
		|g(i,t)-f(i,t)| < (1-2^{-n})\epsilon
		\] 
		for all $(i,t)\in K$ (in particular with $(i,t)\in H$). Conversely, suppose $g$ is in the set on the right. Then since $g$ is continuous and $H$ is dense in $K$, we find that for some $n\geq 1$,
		\[
		|g(i,t)-f(i,t)| \leq (1-2^{-n})\epsilon < \epsilon
		\]
		for all $(i,t)\in K$. Hence $g\in B_K(f,\epsilon)$. This proves the claim. Since $H$ is countable, $B_K(f,\epsilon)$ is formed from countably many unions and intersections of sets in $\mathcal{S}$, thus $B_K(f,\epsilon)\in\sigma(\mathcal{S})$.
		
		Now by problem 1, the topology generated by the basis $\mathcal{A} = \{B_K(f,\epsilon)\}$ is separable and metrizable. The balls of rational radii centered at points of a countable dense subset then give a (different) countable basis $\mathcal{B}$ for the same topology. We claim that this implies that every open set is a \textit{countable} union of sets $B_K(f,\epsilon)$. To see this, let $B\in\mathcal{B}$, and write $B=\bigcup_{\alpha\in I} A_\alpha$, for sets $A_\alpha\in\mathcal{A}$. Then for each $x\in B$, pick $\alpha_x \in I$ such that $x\in A_{\alpha_x}$. Since $\mathcal{B}$ is a basis, there is a set $B_x \in \mathcal{B}$ with $x\in B_x\subseteq A_{\alpha_x}$. Then $B = \bigcup_{x\in B} A_{\alpha_x}$. Note that if $y\in B_y \subseteq A_{\alpha_y}$ and $B_y=B_x$, then in fact $y\in A_{\alpha_x}$, so we can remove $A_{\alpha_y}$ from the union. In other words, we can choose the $A_{\alpha_x}$ so that each corresponds to exactly one $B_x$. But there are only countably many distinct sets $B_x$, so we see that $B$ is a countable union of elements of $\mathcal{A}$. Since every open set can be written as a countable union of elements of $B$, this proves the claim. Since $\mathcal{A}\subseteq\sigma(\mathcal{S})$ by the above, it follows that every open set is in $\sigma(\mathcal{S})$, and consequently so is every Borel set, i.e., $\mathcal{C}_\Sigma \subseteq \sigma(\mathcal{S})$.
		
		In summary, we have shown that the collection $\mathcal{S}$ is a $\pi$-system generating $\mathcal{C}_\Sigma$, so the probability measure $\mathbb{P}\circ X^{-1}$ on $\mathcal{C}_\Sigma$ is uniquely determined by its restriction to $\mathcal{S}$. Suppose
		\begin{align*}
		&\mathbb{P}\left(\{\omega\in\Omega : X(\omega)(i_1,t_1)\leq x_1,\dots,X(\omega)(i_n,t_n)\leq x_n\}\right) =\\
		&\qquad\qquad \mathbb{P}\left(\{\omega\in\Omega : Y(\omega)(i_1,t_1)\leq x_1,\dots,Y(\omega)(i_n,t_n)\leq x_n\}\right)
		\end{align*}
		for all $(i_1,t_1), x_1,\dots,x_n$. This says that the two probability measures $\mathbb{P}\circ X^{-1}$ and $\mathbb{P}\circ Y^{-1}$ agree on $\mathcal{S}$. Then they must agree on all of $\mathcal{C}_\Sigma$, i.e.,
		\[
		\mathbb{P}\left(\{\omega\in\Omega : X(\omega)\in A\}\right) = \mathbb{P}\left(\{\omega\in\Omega : Y(\omega)\in A\}\right)
		\]
		for all $A\in\mathcal{C}_\Sigma$. In other words, the law of a line ensemble is determined by its finite dimensional distributions.
		

\section{Algebra}

	\subsection*{Problem 3}

a.) Show that $\det V = P_V$. \\ 

\textbf{Proof by induction:} Let 
$$V_n = det \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\ 1 &x_2 & x_2^2 & \cdots & x_2^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 &x_n & x_n^2 & \cdots & x_n^{n-1} \end{bmatrix}$$
For all $n \in \mathbb{N}$, we let $P(n)$ be the proposition that $V_n = \prod_{1 \leq i < j \leq n} (x_j - x_i)$.
We note that $det [1] = 1$, so $P(1)$ holds. For the base case, we note that 
$$V_2 = det \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \end{bmatrix}$$
so $V_2 = x_2 - x_1$, so $P(2)$ holds as well. \\

Now, we want to show that if $P(k), k \geq 2$ is true, then $P(k+1)$ must also be true, i.e. for the induction hypothesis
$$V_k = \prod_{1 \leq i < j \leq k} (x_j - x_i)$$
we want to show that
$$V_{k+1} = \prod_{1 \leq i < j \leq k+1} (x_j - x_i)$$\\

For the induction step, consider 
$$V_{k+1} = det \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{k} \\ 1 &x_2 & x_2^2 & \cdots & x_2^{k} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 &x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k} \end{bmatrix}$$
In the first case, suppose $x_i = 0$ for some $i$. Without loss of generality, assume that $x_1 = 0$. Then 
\begin{align*} 
\begin{bmatrix} 1 & 0 & 0 & \cdots & 0 \\ 1 &x_2 & x_2^2 & \cdots & x_2^{k} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 &x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k} \end{bmatrix} &= det \begin{bmatrix} x_2 & x_2^2 & \cdots & x_2^{k} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k} \end{bmatrix}\\
&= x_2 \cdots x_{k+1} det \begin{bmatrix} 1 &x_2 & x_2^2 & \cdots & x_2^{k-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k-1} \end{bmatrix}
\end{align*}
but this is just
$$(x_2 - 0) \cdots (x_{k+1} - 0) det V(x_2, \cdots, x_{k+1}) $$
where $V(x_2, \cdots, x_{k+1})$ is the Vandermonde matrix 
$$\begin{bmatrix} 1 &x_2 & x_2^2 & \cdots & x_2^{k-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k-1} \end{bmatrix}$$\\

In the second case, suppose $x_i \neq 0$ for all $i$ and $x_i$ all distinct. Let
$$Q(a) = det \begin{bmatrix} 1 & a & a^2 & \cdots & a^{k} \\ 1 &x_2 & x_2^2 & \cdots & x_2^{k} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 &x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k} \end{bmatrix}$$
then the polynomial $Q(a)$ has degree at most $k$. For the roots of this polynomial, note that $Q(x_i) = 0$ ($i \geq 2$), so $Q(a) = C(x_2, \cdots, x_{k+1}) (a - x_2) \cdots (a - x_{k+1})$, where $C(x_2, \cdots, x_{k+1})$ is a constant that only depends on $x_2, \cdots, x_{k+1}$. Then $Q(0) = (-1)^k x_2 \cdots x_{k+1} C(x_2, \cdots, x_{k+1})$.\\

Note that we also have that
$$ Q(0) = det \begin{bmatrix} 1 & 0 & 0 & \cdots & 0 \\ 1 &x_2 & x_2^2 & \cdots & x_2^{k} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 &x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k} \end{bmatrix} = det \begin{bmatrix}  x_2 & x_2^2 & \cdots & x_2^{k} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{k+1} & x_{k+1}^2 & \cdots & x_{k+1}^{k} \end{bmatrix}$$
but this is just
$$x_2 \cdots x_{k+1} det V(x_2, \cdots , x_{k+1})$$
so we have that
$$C(x_2, \cdots, x_{k+1}) = (-1)^k det V(x_2, \cdots , x_{k+1})$$
which tells us that
$$Q(a) = (-1)^k det V(x_2, \cdots , x_{k+1}) (a - x_2) \cdots (a - x_{k+1}) = det V(x_2, \cdots , x_{k+1}) (x_2 - a) \cdots (x_{k+1} - a)$$\\

Now note that 
$$det V(x_1, \cdots, x_{k+1}) = Q(x_1) = (x_2 - x_1)(x_3 - x_1) \cdots (x_{k+1} - x_1) det V(x_2, \cdots, x_{k+1})$$
so by induction hypothesis, we are done. \\

b.) Prove that $P_V$ is skew-symmetric. \\

$P$ is skew-symmetric if for all $\sigma$ in $S_n$,
$\sigma(P) = (-1)^\sigma P$. \\

Let $M$ be a Vandermonde matrix. Since $S_n$ is generated by two-cycles $(i, j)$, $i \neq j$, it is enough to show that $\sigma(P) = -P$ for all two-cycles $\sigma$. \\

We know that for vectors $v_i \in \mathbb{R}^n$, $det(v_1, \cdots, v_i, v_j, \cdots, v_n) = -det(v_1, \cdots, v_j, v_i, \cdots, v_n)$, so $$\sigma(P_V)= det(\sigma(M)) =  -det(M) = -P_V.$$\\

c.) Show that if $P$ is any skew-symmetric polynomial in $\mathbb{R}[x_1, \dots, x_n]$ then $P = P_V \cdot Q$, where $Q \in \mathbb{R}[x_1, \dots, x_n]$ is a symmetric polynomial.\\

For some $Q \in Frac(\mathbb{R}[x_1…, x_n])$, we have $P = P_V\cdot Q$, i.e., $Q = P/P_V$. Then $Q$ is symmetric because 
$$\sigma(Q)  = \sigma(P/P_V ) = \sigma(P)/\sigma(P_V) 
= (-1)^\sigma P / ( (-1)^\sigma P_V) = P/P_V$$\\

For any $(x_j - x_i)$ where $j \neq i$, $(x_j - x_i)$ divides $P_V$. 
We want to show that $(x_j - x_i)$ divides $P$ as well for each $(i,j)$ such that $i \neq j$.\\

We know that if $R$ is a UFD, $p_1$ and $p_2$ are irreducible, $(p_1) \neq (p_2)$, then if both $p_1$ and $p_2$ divide $r$ ($r$ in $R$), then $p_1p_2$ divides $r$.\\

Since $\mathbb{R}[x_1, \dots, x_n]$ is a UFD, and each polynomial $(x_j - x_i)$ is irreducible, for $(i, j)$ not equal to $(k, l)$, 
$(x_k - x_l)$ is coprime to $(x_j -x_i)$. Thus if each $(x_j-x_i)$ divides $P$, then $P_V = \prod_{i < j} (x_j - x_i)$ divides $P$. Now let
$$S = \mathbb{R}[x_1, x_2, \dots, x_{j-1}, x_{j+1} , \dots, x_n],$$
so that
$$\mathbb{R}[x_1, \dots, x_n] = S[x_j],$$
so $P$ is a skew-symmetric polynomial in $S[x_j]$.\\

To show that $x_j-x_i$ divides $P$, note that $S = \mathbb{R}[x_1,\dots,x_n]/(x_j-x_i)$. Thus the quotient homomorphism 
\begin{align*}
q &: \mathbb{R}[x_1,\dots,x_n] \longrightarrow S,\\
Q &\mapsto Q(x_1,\dots,x_i,\dots,x_{j-1},x_i,x_{j+1},\dots,x_n).
\end{align*}
has kernel $(x_j - x_i)$.\\

Since $P$ is skew-symmetric, evaluating $P$ at $x_j = x_i$, we find
$$P (x_1, x_2, \dots, x_i, \dots , x_i, \dots, x_n) 
= -P(x_1, x_2, \dots, x_i, \dots, x_i, \dots, x_n),$$
so
$$P(x_1, x_2, \dots, x_i, \dots, x_i, \dots, x_n) =  0.$$
That is, $q(P) = 0$, so $P\in \mathrm{ker}(q)$ and thus $(x_j - x_i)$  divides $P$.\\

It follows from the argument above that $Q = P/P_V$ is in $\mathbb{R}[x_1,\dots,x_n]$, so $Q$ is a symmetric polynomial satisfying $P = P_V\cdot Q$.\\\\

	\subsection*{Problem 4}

a.) Prove that 
\[
s_\lambda(x_1,\dots,x_n) = \begin{dcases}
\frac{\det [x_i^{\lambda_j+n-j}]_{i,j=1}^n}{\det [x_i^{n-j}]_{i,j=1}^n}, & \lambda_{n+1}=0,\\
0, & \lambda_{n+1}\geq 1
\end{dcases}
\]
are symmetric polynomials that are homogeneous and compute their degree.\\ 

Note that in both the numerator and denominator, if we swap two variables $x_i$ and $x_j$, then we are simply swapping two rows in the matrices. This introduces minus signs in both determinants, which cancel when we divide. Since the 2-cycles generate $S_n$, this proves that $s_\lambda$ is symmetric.\\


Consider $[a_{ij}]$. From the determinant expansion formula, we know that the determinant is $$\sum(-1)^\sigma \prod _i a_{i, \sigma(i)}$$\\

Let $M = [x_i^{\lambda_j + n - j}]_{i,j=1}^n$ be the matrix in the numerator. Then from the determinant expansion for $M$, each term of the numerator has the same degree: $\sum_{j=1}^n (\lambda_j + n - j)$. Consider the degree of the denominator (special case where $\lambda_j = 0$ for each $j$ ): the degree is $\sum_{j=1}^n (n - j)$.\\

Then the degree of $s_\lambda$ is $\sum_{j=1}^n (\lambda_j + n - j) - \sum _{j=1}^n (n - j) = \sum_{j=1}^n \lambda_j$. \\ \\


b.) Compute $s_\lambda(1, q, \cdots, q^{n-1}
)$ and use that formula to compute $s_\lambda (1, \cdots, 1)$.\\

Notice that in general, the matrix in the numerator is given by

$$\begin{bmatrix} x_1^{\lambda_1 + n - 1} & x_1^{\lambda_2 + n - 2} &  \cdots & x_1^{\lambda_n} \\
x_2^{\lambda_1 + n - 1} & x_2^{\lambda_2 + n - 2} & \cdots & x_2^{\lambda_n} \\
\vdots & \vdots & \ddots & \vdots \\
x_n^{\lambda_1 + n - 1} & \cdots & \cdots & x_n^{\lambda_n} 
\end{bmatrix}$$

Now, if $(x_1, x_2, \cdots, x_n) = (1, q, q^2, \cdots, q^{n-1})$, then the above matrix becomes

$$\begin{bmatrix} 1^{\lambda_1 + n - 1} & 1^{\lambda_2 + n - 2} &  \cdots & 1^{\lambda_n} \\
q^{\lambda_1 + n - 1} & q^{\lambda_2 + n - 2} & \cdots & q^{\lambda_n} \\
\vdots & \vdots & \ddots & \vdots \\
q^{(n-1)(\lambda_1 + n - 1)} & \cdots & \cdots & q^{(n-1)(\lambda_n)} 
\end{bmatrix}
$$

Note that the $j$-th row is given by $q^{j(\lambda_k + n - k)}  = (q^{\lambda_k + n - k})^j$. Let $r_k = q^{\lambda_k + n - k}$. Then we know that the matrix

$$\begin{bmatrix} 1 & 1 & \cdots & 1 \\
r_1 & r_2 & \cdots & r_k \\
\vdots & \vdots & \ddots & \vdots \\
r_1^{n-1} & \cdots & \cdots & r_k^{n-1} 
\end{bmatrix} $$

which is the transpose of $V(r_1, r_2, \dots, r_n)$. Hence, $det V = P_V (r_1, r_2, \cdots, r_n) = \prod_{i < j} (r_j - r_i) = \prod _{i < j} ( q^{\lambda_j + n - j} - q^{\lambda_i + n - i} )$. Then 
$$s_\lambda(1, q, q^2, \cdots, q^{n-1}) = (-1)^H \frac{\prod_{i < j} (q^{\lambda_j + n - j} - q^{\lambda_i + n - i} )}{\prod_{i<j} (q^j - q^ i)}$$
where $H = \frac{n(n-1)}{2}$.\\

To find $s_\lambda(1, 1, \cdots, 1)$, plug in $q = 1$ and reduce all factors of $(q-1)$ from numerator and denominator:

$$q^m - q^n  = q^n(q^{m-n} - 1)$$
$$f(q) = \frac{q^m - q^n}{q-1} = q^n(1 + q + q^2 + \cdots + q^{m -n -1} )$$
$$f(1) = 1 + 1 + 1 + \cdots + 1 = m - n$$
$$\frac{\prod_{i < j} (q^{\lambda_j + n - j} - q^{\lambda_i + n - i} )}{
\prod_{i, j} (q^j - q^ i)} =  \prod_{i < j} \frac{(q^{\lambda_j + n - j} - q^{\lambda_i + n - i} )}{(q - 1)}\cdot\frac{\prod_{i<j}(q - 1)}{\prod_{i, j} (q^j - q^i)}$$
$$F_{i, j}(q)  = \frac{(q^j - q^i)}{(q - 1)}$$
$$G_{i, j}(q) = \frac{(q^{\lambda_j + n - j} - q^{\lambda_i + n - i})}{ (q - 1)}$$ 
$$F_{i, j}(1) = \lim_{q\to 1} F_{i,j}(q) = j - i $$
$$G_{i, j}(1)  = \lim_{q\to 1} G_{i,j}(q) = \lambda_j - \lambda_i + i - j.$$
The last two lines use l'Hospital's rule.

Then we have that
$$s_\lambda(1,\dots,1) = \frac{\prod_{i< j} G_{i, j}(1)}{\prod_{i< j} F_{i, j}(1)}
= (-1)^{n(n-1)/2} \frac{\prod_{i< j}  (\lambda_j - \lambda_i + (i - j))}{\prod _{i < j} ( j- i)}$$
$$= (-1)^{n(n-1)/2} (-1)^{n(n-1)/2} \frac{\prod_{i< j}  (\lambda_i - \lambda_j + (j -i))}{\prod _{i < j} (j- i)} = (-1)^{n(n-1)} \frac{\prod_{i< j}  (\lambda_i - \lambda_j + (j-i))}{\prod _{i < j } ( j- i)}$$
$$= \prod_{i< j}  \frac{\lambda_i - \lambda_j + j -i}{j-i},$$
since $n(n-1)$ is always even.


\section{Weak convergence}


	\subsection*{Problem 5}
	(1)$\phi_{n}(t)=\mathbb{E}[e^{itY_{n}}]=\sum\limits_{k=0}^{\infty}p_{n}(1-p_{n})^{k}e^{itp_{n}k}=\frac{p_{n}}{1-(1-p_{n})e^{itp_{n}}}$. Then, $$\lim\limits_{n\rightarrow\infty}\phi_{n}(t)=\lim\limits_{x\rightarrow 0}\frac{x}{1-(1-x)e^{itx}}=\lim\limits_{x\rightarrow 0}\frac{1}{1-it(1-x)e^{itx}}(\text{L'Hospital})=\frac{1}{1-it},$$
which is the characteristic function of exponential random variable with parameter $1$. Therefore, $Y_{n}$ weakly converges to $Z\sim Exp(1)$.\\
(2) Notice that 
\begin{align*}
\frac{d}{d q_{n}}\mathbb{E}[Y_{n}^{k-1}]&=\frac{d}{d q_{n}}[\sum_{x=0}^{\infty}p_{n}^{k-1}x^{k-1}p_{n}q_{n}^{x}]=	\sum_{x=0}^{\infty}x^{k-1}[-kp_{n}^{k-1}q_{n}^{x}+p_{n}^{k}x q_{n}^{x-1}]\\
&=-\frac{k}{p_{n}}\sum_{x=0}^{\infty}(p_{n}x)^{k-1}p_{n}q_{n}^{x}+\frac{1}{p_{n}q_{n}}\sum_{x=0}^{\infty}(p_{n}x)^{k}p_{n}q_{n}^{x}\\
&=-\frac{k}{p_{n}}\mathbb{E}[Y_{n}^{k-1}]+\frac{1}{p_{n}q_{n}}\mathbb{E}[Y_{n}^{k}]
\end{align*}
Therefore, we have $$\mathbb{E}[Y_{n}^{k}]=p_{n}q_{n}\frac{d}{d q_{n}}\mathbb{E}[Y_{n}^{k-1}]+k\cdot q_{n}\mathbb{E}[Y^{k-1}]$$
Let $p_{n}\rightarrow 0$, we get $\lim\limits_{n\rightarrow\infty}\mathbb{E}[Y_{n}^{k}]=k\cdot \lim\limits_{n\rightarrow\infty}\mathbb{E}[Y_{n}^{k-1}]$.
Since $\lim\limits_{n\rightarrow\infty}\mathbb{E}[Y_{n}]=\lim\limits_{n\rightarrow\infty}p_{n}\cdot\frac{1-p_{n}}{p_{n}}=1$, we obtain: $$\lim\limits_{n\rightarrow\infty}\mathbb{E}[Y_{n}^k]=k!$$
which is the $k$-$th$ moment of exponential random variable with parameter $1$.


(3) For a bounded continuous function $f$ which is bounded by $M$, $$\mathbb{E}[f(Y_{n})]=\sum_{k=0}^{\infty}f(kp_{n})p_{n}(1-p_{n})^{k}\leqslant \frac{M(1-p_{n})}{p_{n}}$$ is well-defined.
Notice that $(1-p_{n})^{k}=e^{kln(1-p_{n})}=e^{-kp_{n}+o(p_{n})}=e^{-kp_{n}}(1+o(p_{n}))$, so $$\mathbb{E}[f(Y_{n})]=\sum_{k=0}^{\infty}f(kp_{n})p_{n}e^{-kp_{n}}+\sum_{k=0}^{\infty}f(kp_{n})p_{n}e^{-kp_{n}}o(p_{n})$$
For the first term, $$\lim_{n\rightarrow\infty}\sum_{k=0}^{\infty}f(kp_{n})p_{n}e^{-kp_{n}}=\int_{0}^{\infty}f(x)e^{-x}dx=\mathbb{E}[f(Y)]$$
by definition of integral, and here we use the continuity of function $f$. For the second term, it converges to $0$. Thus, $\mathbb{E}[f(Y_{n})]\xrightarrow{n\rightarrow\infty}\mathbb{E}[f(Y)]$.\\
(4) Consider $\frac{1}{p_{n}}\cdot p_{n}(1-p_{n})^{k_{n}}$, where $k_{n}=x\cdot\frac{1}{p_{n}}$. Notice that $\frac{1}{p_{n}}\cdot p_{n}(1-p_{n})^{k_{n}}=e^{\frac{x}{p_{n}}ln(1-p_{n})}=e^{\frac{x}{p_{n}}}(-p_{n}+o(p_{n}))=e^{-x+o(1)}$. Consider 
\begin{align*}
	\mathbb{P}(a\leqslant Y_{n}\leqslant b) &= \mathbb{P}(\frac{a}{p_{n}}\leqslant X_{n}\leqslant \frac{b}{p_{n}})\\
	&=\sum_{k=m_{n}}^{M_{n}}\mathbb{P}(X_{n}=k)\quad(\text{where $m_n=[\frac{a}{p_{n}}]+1$, $M_{n}=[\frac{b}{p_{n}}]$})\\
	&= \sum_{k=m_{n}}^{M_{n}}p_{n}e^{x_{k}+o(1)}\quad(\text{where $x_{k}=p_{n}k$ and $x_{k}-x_{k-1}=p_{n}$})\\
	&\approx\sum_{k=m_{n}}^{M_{n}}\int_{x_{k}-\frac{1}{2}p_{n}}^{x_{k}+\frac{1}{2}p_{n}}e^{-x}dx=\int_{x_{m_{n}}-\frac{1}{2}p_{n}}^{x_{M_{n}}+\frac{1}{2}p_{n}}e^{-x}dx\\
	& \rightarrow \int_{a}^{b}e^{-x}dx\quad(\text{as $n\rightarrow\infty$})
\end{align*}
Therefore, $\lim\limits_{n\rightarrow\infty}\mathbb{P}(Y_{n}\leqslant x)=\int_{-\infty}^{x}e^{-u}du$.

	\subsection*{Problem 6}
	(1)\begin{align*}
	\phi_{n}(t)&= \mathbb{E}[e^{itX_{n}}]=\sum_{k=0}^{N_{n}}\binom{N_{n}}{k}p_n^{k}(1-p_n)^{N_{n}-k}e^{itk}\\
	&=(p_{n}e^{it}+(1-p_n))^{N_{n}}\\
	&=e^{N_{n}ln(1+p_{n}(e^{it}-1))}
\end{align*}
As $p_n\rightarrow 0$, $N_{n}\rightarrow\infty$, $p_{n}N_{n}\rightarrow \lambda$, we have $ln(1+p_{n}(e^{it}-1))\rightarrow p_{n}(e^{it}-1)$, and $\lim\limits_{n\rightarrow\infty}\phi_{n}(t)= e^{\lim\limits_{n\rightarrow\infty}N_{n}p_{n}(e^{it}-1)}=e^{\lambda (e^{it}-1)}$, which is the characteristic function of Poisson distribution. Thus, $X_{n}$ weakly converges to Poisson random variable with parameter $\lambda$.\\
(2) Denote $$P_{k,n}=\frac{N_{n}!}{k!(N_{n}-k)!}\cdot p_{n}^{k}(1-p_{n})^{N_{n}-k}=\frac{(p_{n}N_{n})^{k}}{k!}\cdot \frac{N_{n}!}{N_{n}^{k}(N_{n}-k)!}(1-p_{n})^{N_{n}-k}$$
Notice that $\frac{N_{n}!}{N_{n}^{k}(N_{n}-k)!}=\frac{N_{n}}{N_{n}}\cdot\frac{N_{n}-1}{N_{n}}\cdot\dots\cdot\frac{N_{n}-k+1}{N_{n}}\rightarrow 1$, as $N_{n}\rightarrow\infty$;\\ 

	$(1-p_{n})^{N_{n}-k}=e^{(N_{n}-k)ln(1-p_{n})}=e^{(N_{n}-k)(-p_{n}+o(p_{n}))}\rightarrow e^{-\lambda}$, as $n\rightarrow\infty$; \\ and $\frac{(p_{n}N_{n})^{k}}{k!}\rightarrow\frac{\lambda^{k}}{k!}$. Therefore, $P_{k,n}\rightarrow\frac{\lambda^{k}}{k!}e^{-\lambda}$ as $n\rightarrow\infty$. Then, $\mathbb{P}(X_{n}\leqslant x)=\sum_{k=1}^{[x]}P_{k,n}$. Let $n\rightarrow\infty$, $\mathbb{P}(X_{n}\leqslant x)=\sum_{k=1}^{[x]}P_{k,n}\rightarrow\sum_{k=1}^{[x]}\frac{\lambda^{k}}{k!}e^{-\lambda}$ is the distribution of Poisson random variable.

	\subsection*{Problem 7}
(1)
\begin{align*}
\phi_{n}(t)&=\mathbb{E}[e^{itY_{n}}]=\sum_{k=0}^{\infty}e^{-n}\frac{n^k}{k!}e^{it\frac{k-n}{\sqrt{n}}}\\
&=\sum_{k=0}^{\infty}\frac{(ne^{it\frac{1}{\sqrt{n}}})^{k}}{k!}e^{-it\sqrt{n}-n}\\
&=e^{-it\sqrt{n}-n+ne^{it\frac{1}{\sqrt{n}}}}
\end{align*}
Notice that $n(e^{it\frac{1}{\sqrt{n}}}-1)-it\sqrt{n}=n(it\frac{1}{\sqrt{n}}+\frac{1}{2}(it\frac{1}{\sqrt{n}})^2+o(\frac{1}{n}))-it\sqrt{n}=-\frac{1}{2}t^2+o(1)$. Therefore, $\phi_{n}(t)\rightarrow e^{-\frac{1}{2}t^2}$ as $n\rightarrow\infty$, which is the characteristic function of standard normal random variable.\\
(2) Let us consider $\lim\limits_{n\rightarrow\infty}\sqrt{n}\frac{n^{k_{n}}}{k_{n}!}e^{-n}$, where $k_{n}=x\sqrt{n}+n$. By Stirling's formula, $n!\sim\sqrt{2\pi n}n^{n}e^{-n}$. Then,
\begin{align*}
\sqrt{n}\frac{n^{k_{n}}}{k_{n}!}e^{-n} &\sim \sqrt{n}\frac{n^{k_{n}}}{\sqrt{2\pi k_{n}}k_{n}^{k_{n}}e^{-k_{n}}} e^{-n}\\
&=\frac{\sqrt{n}}{\sqrt{2\pi k_{n}}}(\frac{n}{k_{n}})^{k_{n}}e^{k_{n}-n}\\
&=\frac{\sqrt{n}}{\sqrt{2\pi k_{n}}}e^{k_{n}ln(\frac{n}{k_{n}})+k_{n}-n}
\end{align*}
Notice that $k_{n}=x\sqrt{n}+n\sim O(n)$, we know $\lim\limits_{n\rightarrow\infty}\frac{\sqrt{n}}{\sqrt{k_{n}}}=1$;
\begin{align*}
	k_{n}ln(\frac{n}{k_{n}})&=k_{n}ln(1-\frac{k_{n}-n}{k_{n}})\quad(\frac{k_{n}-n}{k_{n}}=\frac{x}{x+\sqrt{n}}\sim O(\frac{1}{\sqrt{n}}))\\
	&=k_{n}(-\frac{k_{n}-n}{k_{n}}-\frac{1}{2}(\frac{k_{n}-n}{k_{n}})^{2}+o(\frac{1}{n}))\\
	&=-k_{n}+n-\frac{1}{2}\frac{n x^2}{x\sqrt{n}+n}+o(1)\\
	&=-k_{n}+n-\frac{1}{2}x^2+o(1)
\end{align*}
Therefore, $\sqrt{n}\frac{n^{k_{n}}}{k_{n}!}e^{-n}=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2+o(1)}$.\\
Next, consider: $\mathbb{P}(a\leqslant Y_{n}\leqslant b)=\mathbb{P}(a\sqrt{n}+n\leqslant X_{n}\leqslant b_{n}+n)$. Denote $m_{n}=[a\sqrt{n}+n]+1$, $M_{n}=[b\sqrt{n}+n]$, then
\begin{align*}
	\mathbb{P}(a\leqslant Y_{n}\leqslant b)&=\sum_{k=m_{n}}^{M_{n}}\mathbb{P}(X_{n}=k)\\
	&=\sum_{k=m_{n}}^{M_{n}}\frac{1}{\sqrt{n}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x_{k}^2}{2}+o(1)}\quad(\text{where $x_{k}=\frac{k-n}{\sqrt{n}}$, $x_{k}-x_{k-1}=\frac{1}{\sqrt{n}}$})\\
	&\approx \sum_{k=m_{n}}^{M_{n}}\int_{x_{k}-\frac{1}{2\sqrt{n}}}^{x_{k}+\frac{1}{2\sqrt{n}}} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\
	&=\int_{x_{m_{n}}-\frac{1}{2\sqrt{n}}}^{x_{M_{n}}+\frac{1}{2\sqrt{n}}} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\
	& \rightarrow \int_{a}^{b}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx
\end{align*}
Therefore, $\lim\limits_{n\rightarrow\infty}\mathbb{P}(Y_{n}\leqslant x)=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}}du$.\\
(3) Suppose $Z_{1},Z_{2},\dots, Z_{n}$, \emph{I.I.D}, are Poisson random variables with parameter $1$. Then, $X_{n}=\sum\limits_{k=1}^{n}Z_{k}\sim Poisson(n)$, and $\mathbb{E}(X_{n})=n$, $Var(X_{n})=n$. By Central Limit Theorem, $\frac{X_{n}-n}{\sqrt{n}}\xrightarrow{d}\mathcal{N}(0,1)$.

\section{Tightness}

	\subsection*{Problem 8}
	
		Let $\Lambda\subset\mathbb{R}$ be an interval and $\Sigma = \llbracket 1, N\rrbracket$ with $N\in\mathbb{N}\cup\{\infty\}$. Consider the maps 
		\[
		\pi_i : C(\Sigma\times\Lambda) \to C(\Lambda), \quad \pi_i(F)(x) = F(i,x), \quad i\in\Sigma.
		\]
		Since $C(X)$ with the topology of compact convergence is metrizable by problem 1, to show that the $\pi_i$ are continuous, it suffices to show that if $f_n\to f$ in $C(\Sigma\times\Lambda)$, then $\pi_i(f_n)\to \pi_i(f)$ in $C(\Lambda)$. But this is immediate, since if $f_n\to f$ uniformly on compact subsets of $\Sigma\times\Lambda$, then in particular $f_n(i,\cdot)\to f(i,\cdot)$ uniformly on compact subsets of $\Lambda$.
		
		Let $(\mathcal{L}^n)$ be a sequence of $\Sigma$-indexed line ensembles on $\Lambda$, i.e., each $\mathcal{L}^n$ is a $C(\Sigma\times\Lambda)$-valued random variable on a probability space $(\Omega,\mathcal{F},\mathbb{P})$. Let $X_i^n := \pi_i(\mathcal{L}^n)$. If $A$ is a Borel set in $C(\Lambda)$, then $(X_i^n)^{-1}(A) = (\mathcal{L}^n)^{-1}(\pi_i^{-1}(A))$. Note $\pi_i^{-1}(A)\in\mathcal{C}_\Sigma$ since $\pi_i$ is continuous, so it follows that $(X_i^n)^{-1}(A)\in\mathcal{F}$. Thus $X_i^n$ is a $C(\Lambda)$-valued random variable.
		
		Suppose the sequence $(\mathcal{L}^n)$ is tight. Then $(\mathcal{L}^n)$ is relatively compact, that is, every subsequence $(\mathcal{L}^{n_k})$ has a further subsequence $(\mathcal{L}^{n_{k_\ell}})$ converging weakly to some $\mathcal{L}$. Then for each $i\in\Sigma$, since $\pi_i$ is continuous, the subsequence $(\pi_i(\mathcal{L}^{n_{k_\ell}}))$ of $(\pi_i(\mathcal{L}^{n_k}))$ converges weakly to $\pi_i(\mathcal{L})$ by the continuous mapping theorem. Thus every subsequence of $(\pi_i(\mathcal{L}^n))$ has a convergent subsequence. Since $C(\Lambda)$ is a Polish space by the argument in problem 1, Prohorov's theorem implies that each $(\pi_i(\mathcal{L}^n))$ is tight.
		
		Conversely, suppose $(\pi_i(\mathcal{L}^n))$ is tight for all $i\in\Sigma$. Then for each $i$, every subsequence $(\pi_i(\mathcal{L}^{n_k}))$ has a further subsequence $(\pi_i(\mathcal{L}^{n_{k_\ell}}))$ converging weakly to some $\mathcal{L}_i$. By diagonalizing the subsequences $(n_{k_\ell})$, we obtain a sequence that works for all $i$, so that $\pi_i(\mathcal{L}^{n_{k_\ell}})\implies \mathcal{L}_i$ for all $i$ simultaneously. Note that $C(\Sigma\times\Lambda)$ is homeomorphic to $\prod_{i\in\Sigma} C(\Lambda)$ with the product topology, with $f\in C(\Sigma\times\Lambda)$ identified with $(\pi_i(f))_{i\in\Sigma}$. It is not hard to see this by observing that the compact subsets $K$ of $\Sigma\times\Lambda$ are of the form $S\times I$, for $S$ finite and $I$ compact. Thus the homeomorphism identifies the basis elements $B_K(f,\epsilon)$ in $C(\Sigma\times\Lambda)$ with products of open sets $U_i$ in $C(\Lambda)$, such that if $i\notin S$ then simply $U_i = C(\Lambda)$; since $S$ is finite, these products $\prod_i U_i$ are basis elements of the product topology.
				 
		Consequently, we can identify the sequence of random variables $\mathcal{L} = (\mathcal{L}_i)_{i\in\Sigma}$ with an element of $C(\Sigma\times\Lambda)$. We argue that $\mathcal{L}^{n_{k_\ell}}\implies \mathcal{L}$. Let $U$ be a basis element in the product topology, i.e., $U = \prod_{i\in\Sigma} U_i$, with each $U_i$ open in $C(\Lambda)$ and all but finitely many $U_i = C(\Lambda)$. Without loss of generality, assume these finitely many $U_i\neq C(\Lambda)$ are $U_1,\dots,U_m$. Then
		\[
		\mathbb{P}(X \in U) = \mathbb{P}(\pi_1(X) \in U_1, \dots, \pi_m(X) \in U_m) = \prod_{i=1}^m \mathbb{P}(\pi_i(X)\in U_i).
		\]
		Therefore, since $\pi_i(\mathcal{L}^{n_{k_\ell}}) \implies \mathcal{L}_i$ for each $i$,
		\begin{align*}
		\liminf_{\ell\to\infty} \mathbb{P}(\mathcal{L}^{n_{k_\ell}} \in U) &\geq \prod_{i=1}^m \liminf_{\ell\to\infty} \mathbb{P}(\pi_i(\mathcal{L}^{n_{k_\ell}})\in U_i) \geq \prod_{i=1}^m \mathbb{P}(\mathcal{L}_i \in U_i) = \mathbb{P}(\mathcal{L}\in U).
		\end{align*}
		Now by the same argument as in problem 2, since $C(\Sigma\times\Lambda)$ is a second countable metric space, every open set is a union of countably many sets of the form of $U$. It follows from countable additivity that the condition above holds if $U$ is replaced by an arbitrary open set. This proves that $\mathcal{L}^{n_{k_\ell}} \implies \mathcal{L}$ as desired. Hence $(\mathcal{L}^n)$ is relatively compact, and it follows from Prohorov's theorem  once again that $(\mathcal{L}^n)$ is tight. This completes the proof.
		
	
	\subsection*{Problem 9}
	
		Recall that Theorem 7.3 from Billingsley states that a sequence $(P_n)$ of probability measures on $C[0,1]$ with the uniform topology is tight if and only if the following hold:
		\begin{align}
			\lim_{a\to\infty} \limsup_{n\to\infty} P_n(|x(0)|\geq a) &= 0 \\
			\lim_{\delta\to 0} \limsup_{n\to\infty} P_n\left(\sup_{|s-t|\leq\delta} |x(s)-x(t)| \geq \epsilon\right) &= 0, \quad \forall\,\epsilon>0.
		\end{align}
		
		We will find analogous necessary and sufficient conditions for the tightness of $(\mathcal{L}^n)$ on $C(\Sigma\times\Lambda)$ in problem 8. It suffices to find conditions for the tightness of the sequences $(\mathcal{L}^n_i) := (\pi_i(\mathcal{L}^n_i))$ on $C(\Lambda)$, with $i\in\Sigma$. Note $C(\Lambda)$ has the topology of uniform convergence on compact sets, so we must work on the level of compact subsets of $\Lambda$. Consider the compact exhaustion $\Lambda = \bigcup_k [a_k,b_k]$ as in problem 1. Recall that $[a_1,b_1]\subseteq [a_2,b_2]\subseteq\cdots$, so $a_1\in [a_k,b_k]$ for all $k$. We argue that $(\mathcal{L}^n_i)$ is tight if and only if for every $k\geq 1$, we have
		\begin{enumerate}[label=(\roman*)]
			
			\item 
			\[
			\lim_{a\to\infty} \limsup_{n\to\infty}\, \pr(|\mathcal{L}^n_i(a_1)|\geq a) = 0.
			\]
			
			\item For all $\epsilon>0$,
			\[
			\lim_{\delta\to 0} \limsup_{n\to\infty}\, \pr\bigg(\sup_{\substack{x,y\in [a_k,b_k], \\ |x-y|\leq\delta}} |\mathcal{L}^n_i(x) - \mathcal{L}^n_i(y)| \geq \epsilon\bigg) = 0.
			\]
			
		\end{enumerate}
	
		By replacing $[0,1]$ with $[a_k,b_k]$ and 0 with $a_1$, we see by Theorem 7.3 that these conditions imply that the restricted sequences $(\mathcal{L}^n_i|_{[a_k,b_k]})_n$ are tight, hence relatively compact in the uniform topology on $C[a_k,b_k]$, for every $i\in\Sigma$ and $k\geq 1$. Thus every subsequence $(\mathcal{L}^{n_m}_i|_{[a_k,b_k]})_m$ has a further subsequence $(\mathcal{L}^{n_{m_\ell}}_i|_{[a_k,b_k]})_\ell$ converging weakly to some $\mathcal{L}_i^{[a_k,b_k]}$. We claim that we can patch these $\mathcal{L}_i^{[a_k,b_k]}$ together to obtain a well-defined random variable $\mathcal{L}_i$ on all of $C(\Lambda)$, such that $\mathcal{L}_i|_{[a_k,b_k]} = \mathcal{L}_i^{[a_k,b_k]}$ for every $k$. To see this, note that this $\mathcal{L}_i$ is uniquely determined by its fdd's, according to problem 2. Given any finite collection $A=\{x_1,\dots,x_j\}$ of points in $\Lambda$, if we take $k$ large enough so that $A \subset [a_k,b_k]$, then the corresponding fdd $\{\mathcal{L}_i(x_1)\in B_1, \dots, \mathcal{L}_i(x_j) \in B_j\}$ is determined by that of $\mathcal{L}_i^{[a_k,b_k]}$. Moreover, uniqueness of weak limits in distribution implies that this fdd agrees with that of $\mathcal{L}_i^{[a_\ell,b_\ell]}$ whenever $A\subset[a_\ell,b_\ell]$. Thus we have specified well-defined fdd's for $\mathcal{L}_i$, which determines $\mathcal{L}_i$ on all of $C(\Lambda)$. By construction, the restriction of $\mathcal{L}_i$ to any $[a_k,b_k]$ is equal to $\mathcal{L}_i^{[a_k,b_k]}$ in distribution.
		
		In particular, we see that $\mathcal{L}_i^{n_{m_\ell}}|_{[a_k,b_k]} \implies \mathcal{L}_i|_{[a_k,b_k]}$ in the uniform topology on $C[a_k,b_k]$, for every $k$. If $K\subset\Lambda$ is any compact set, then by taking $k$ large enough so that $K\subset [a_k,b_k]$, we also find $\mathcal{L}_i^{n_{m_\ell}}|_K \implies \mathcal{L}_i|_K$ in the uniform topology on $C(K)$. Let $B_K(f,\epsilon)$ be a basis element in $C(\Lambda)$, and let $B_\epsilon(f|_K)$ denote the corresponding ball in the uniform topology on $C(K)$. Then
		\begin{align*}
		\liminf_{\ell\to\infty}\,\mathbb{P}(\mathcal{L}^{n_{m_\ell}}_i \in B_K(f,\epsilon)) &= \liminf_{\ell\to\infty}\,\mathbb{P}(\mathcal{L}^{n_{m_\ell}}_i|_K \in B_\epsilon(f|_K))\\
		& \geq \mathbb{P}(\mathcal{L}_i|_K \in B_\epsilon(f|_K)) = \mathbb{P}(\mathcal{L}_i \in B_K(f,\epsilon)).
		\end{align*}
		The inequality follows from weak convergence in the uniform topology on $C(K)$. Since every open set in $C(\Lambda)$ can be written as a countable union of sets $B_K(f,\epsilon)$ (see problem 2), it follows from countable additivity that
		\[
		\liminf_{\ell\to\infty}\,\mathbb{P}(\mathcal{L}^{n_{m_\ell}}_i \in U) \geq \mathbb{P}(\mathcal{L}_i \in U)
		\]
		for any $U$ open in $C(\Lambda)$. Therefore $(\mathcal{L}_i^{n_{m_\ell}})_\ell$ converges weakly to $\mathcal{L}_i$, proving that $(\mathcal{L}^n_i)_n$ is relatively compact, hence tight, for every $i\in\Sigma$. Therefore $(\mathcal{L}^n)$ is tight by problem 8.


\section{Lozenge tilings of the hexagon}

	\subsection*{Problem 10}
		Let $p(i)$ be the function which gives the number of particles with first coordinate $i$, then $p(i)=i$
		
		Base case: Where $i=0$. We find that at the base case $i=0$, we know that the right hand side of any $i$-coordinate line has type $2$ triangles, and at base case $i=0$,  the left hand side of the line lies outside of the tiling region. 
		Because particles lie at the centers of horizontal (type 1) lozenges, we find that any particle with coordinate $i=0$ would necessitate a type $1$ triangle in a horizontal lozenge outside of the tiling region. Therefore, $p(0)=0$.
		
		Inductive case: Suppose it holds upto $i=k$ that $p(i)=i$. 
		Denote the column made between the coordinate lines $i=k$ and $i=k+1$ and contained within the extended tiling region as $[k,k+1]_T$. 
		The column $[k, k+1]_T$ contains $A+k$ type $2$ triangles and $A+k+1$ type $1$ triangles, since the length of the coordinate line $i=k$ is $A+k$ and the length of the coordinate line $i=k+1$ is $A+k+1$. 
		
	Because $p(k)=k$,  we know that there must be $k$ horizontal (type 1) lozenges spanning the coordinate line $i=k$ and so because each particle will be in a type $1$ lozenge containing one type 2 triangle in $[k,k+1]_T$, of the type $2$ triangles in $[k,k+1]_T$, exactly $k$ type $2$ triangles are in horizontal lozenges and the remaining $A$ type $2$ triangles are in vertical (type 2 or type 3) lozenges. 
		 Vertical lozenges lie entirely within $[k,k+1]_T$ which implies that there are $A$ type 1 triangles filled by verticle lozenges in $[k,k+1]_T$ so there are $A+k+1-A=k+1$ horizontal lozenges beginning in $[k,k+1]_T$, all of which place particles on the coordinate line $i=k+1$. 
		 Therefore the inductive steps hold and we know $p(k+1)=k+1$, so $p(i)=i, \forall i\in \mathbb{N}$
		
		Now additionally we know that there are $A$ unit lengths which are not inside of horizontal lozenges on any coordinate line. 
		These unit lengths are sides of type 2 or type 3 lozenges. 
		If we take the midpoints of these $A$ unit lengths in descending order we may label the second coordinate of their midpoints as $a_j^i$. 
		For each $i$, $j$ ranges between $1$ and $A$, since there are $A$ such lengths adjacent to verticle type 2 and 3 lozenges. 
		Now if we consider the values of $a_j^i$ and $a_j^{i+1}$ we may find that $a_j^{i+1}-a_j^i\in \{0,1\}$. 
		
		As proof for this claim, consider the following: Vertical lozenges ending at $i=k$ has a side with has a type $2$ triangle on its right hand side which must be tiled into a type $2$ or $3$ lozenge, since it is not in a type $1$ lozenge.
		Therefore we find that each vertical (type 2 and 3) lozenge in the column $[k-1,k]_T$ is connected  to another vertical lozenge in the column $[k,k+1]_T$ and that these generate $A$ lozenges since they may not overlap, which exhausts the number of vertical lozenges in $[k,k+1]$. Therefore we find that each $a_j^{i+1}$ is connected to $a_j^{i}$.
		If $a_{j}^{i+1}$ is connected to $a_j^i$ by a type $2$ lozenge, then $a_{j}^{i+1}=a_j^i+1$ and if they are connected by a type 3 lozenge, then $a_j^{i+1}=a_j^i$.
		Therefore we find that $a_j^{i+1}-a_j^i\in \{0,1\}$.
	
		Now, let us prove that $\forall i, y^{i+1}\succ y^i$. i.e. $\forall (i,j)$, we have $y_j^{i+1}>y_j^{i}>y_{j+1}^{i+1}$.
		We will prove this statement inductively over $n$ for $a_{n}^i$, and demonstrating that this property holds for all $y_j^i$ and corresponding other values where $y_j^i>a_n^i$. 
		For the base case, consider $n=1$. We know that for each of these $y_j^i$ we get $y_j^{i+1}=A+j+1-i+0.5=y_j^i+1$ since we can count in the same way as we did previously. 
		This gives us the first inequality for our base case. For the other inequality, we must break into two cases, the first where $a_1^{i+1}=a_1^i$ and the second where $a_1^{i+1}=a_1^i+1$. 
		In the first case, $y_{j+1}^{i+1}=A+(j+1)-(i+1)+0.5=A+j-i+0.5=y_j^i$. 
		In the second case, the inequality is the same for all except for the final such $y_j^i$ in which case we find that $a_1^{i+1}=y_j^i$ while $y_{j+1}^{i+1}<a_1^{i+1}$ and so we have $y_j^{i+1}>y_j^i>y_{j+1}^{i+1}$
		
		Now for the inductive case, assume that $y_j^{i+1}>y_j^{i}>y_{j+1}^{i+1}$ holds for all $y_j^i>a_n^i$ for $n$ upto $k$. 
		If $a_{n+1}^{i}=a_n^i-1$ then the inductive case already holds for all $y_j^i>a_{n+1}^i$ since there would be no space for a particle between $a_n^i$ and $a_{n+1}^i$ so $y_j^i>a_n^i\implies y_j^i>a_{n+1}^i$, and the inductive hypothesis holds. 
		Now if $a_n^i>a_{n+}^i+1$, then there are four possible states for which lozenges appear on the upper right and lower right side of the horizontal lozenge containing the particle at $y_j^i$. 
		
		If $y_j^i$ has a horizontal lozenges above it on the right side, we know by the same continuity of the lines of vertical lozenges as in the base case that $a_{n+1}^{i+1}< y_{j}^{i}+1<a_n^{i+1}$, so we may find that the $j$ value associated with these values by counting down from $A+i+1$. 
		There are $A+i+1$ positions along the line at coordinate $i+1$, and there are $n$ vertical lozenges above $y_j^i+1$ so if it is the $j'$ lozenge, we find that $$A-i-1-n+j'-0.5=y_j^i+1=A-i+j-n-0.5+1$$  
		this implies that $j'= j$ and so $y^{i+1}_{j}=y^{i}_j+1$ and so $y_{j}^{i+1}>y^i_j$. 
		Otherwise, $y_j^i$ has a vertical lozenge (type 3) above it on the right.
		Using the same counting tricks as before, we can find that $y_{j}^{i+1}>a_{n}^{i+1}\geq a_n^i>y_j^i$ in this case, giving us the inequality. Meaning that in all cases, we get $y_{j}^{i+1}$.  
		Now we  know $y_{j}^{i+1}$ is the first horizontal particle such that $y_j^i<y_j^{i+1}$, so then we get that $y_j^i\geq y_{j+1}^{i+1}$, meaning we get that $y_j^{i+1}>y_j^i\geq y_j^{i+1}$. 
		
		In particular this inequality gives us $y_j^{i+1}-i+j-0.5>y_j^i-i+j-0.5>y^{i+1}_{j+1}-i+j-0.5$. 
		This means we find $y_j^{i+1}-i+j-0.5>\lambda_j^i>\lambda_{j+1}^{i+1}$ by the definition of $\lambda_j^i$.
		This also gives us that $\lambda_j^{i+1}>\lambda_j^i-1$, and because they are integers, we find that this implies $\lambda_j^{i+1}\geq\lambda_j^i$, and thus $\lambda^i\preceq \lambda^{i+1}$.
		
		Now we can find that if we let $f$ map $\omega\in \Omega$, the set of possible extended tilings of the $A\times B \times C$ hexagon into $$GT_\mu:=\{(\lambda^1,\cdots,\lambda^{B+C})\mid \lambda^{B+C}=(A^C)=\mu, \lambda^i\in \mathbb{Y}^i, \lambda^i\preceq \lambda^{i+1} \text{ for } i=1, \cdots , B+C-1\}$$ with $f(\omega)=(\lambda^1(\omega), \cdots, \lambda^{B+C}(\omega))$. 
		Let us show that $f$ is a bijection. First, assume there exist $\omega, \omega'$ such that $f(\omega)=f(\omega')$. 
		As we proved earlier in the proof, if $a_n^i>y_j^i>a_{n+1}^i$ then $y_j^i=A+i-j-n+0.5$ and $\lambda_j^i=y_j^i-i+j-0.5$ therefore $A-n=\lambda_j^i$. 
		This means that $\lambda^i$ fixes the positions (and type, using the $i+1$ column $j$th coordinate) of the verticle lozenges, and therefore fixes the positions of the horizontal lozenges, and hence $\omega_i=\omega'_i$ where $\omega_i$ is the section of lozenges which touches the $i$ coordinate line. 
		This implies injectivity. 
		
		Now, for surjectivity, suppose we wish to find an $\omega$ such that $f(\omega)=(\lambda^1, \lambda^2,...,\lambda^{B+C})=T\in GT_\mu$. 
		We may do so in the following manner: $\lambda^i=(A^{k_A}, \cdots 1^{k_1}, 0^{k_0} )$ (all values must fall between $A$ and $0$ bounds are set since $\lambda^B+C=A^C0^B$, and $\lambda_i\preceq \lambda_{i+1}$, and $\sum_{m=0}^A k_m=i$ for each $\lambda_i$ because the $\lambda_i$ interlace.): Starting at the bottom of coordinate line at $i$ with the $k_m$ for $\lambda_i$, recursively place $k_m$ particles and then skip one unit length, then repeat starting from the top of the unit length skipped. 
		If we label this $\omega$, we find that $\lambda_i(\omega)=A^{K_A},\cdots 0^{k_0}$ since $\lambda_j^i=A-n$ and so since we can view each skipped step as an $a_n^i$, we find that these numbers align correctly. Hence, the function is bijective.
		
		
		Finally, if $(\nu)$ ranges over all possible partition sequences $(\nu)=(\nu^{(0)},\nu^{(1)},..,\nu^{(B+C)})$ such that $\nu^{B+C}=\mu = (A^C)$ and $\nu^0=0$ and  then $$s_\mu(1^{(B+C)})=s_{\mu/\nu}(1^{(1)},1^{(2)},...,1^{(B+C)})=\sum_{(\nu)}\prod_{i=1}^n s_{\nu^{(i)}/\nu^{(i-1)}}(1^{(i)})$$
		We know that $s_{\nu^{(i)}/\nu^{(i-1)}}(1^{(i)})=0$ unless $\nu^{(i)}/\nu^{(i-1)}$ is a horizontal strip, in which case it is equal to $x^{|\nu^{i}-\nu^{i-1}|}$ [Macdonald, 72]. 
		We note that $(n)\in GT_\mu$ if and only if each $\nu^{(i)}-\nu^{(i-1)}$ is a horizontal strip, as this occurs if and only if $\nu^{(i)}\succeq \nu^{(i-1)}$. 
		This means that we find that $$s_\mu(1^{(B+C)})=\sum_{(\nu\in GT_\mu)}\prod_{i=1} 1^{|\nu^{(i)}-\nu^{(i-1)}|}=\sum_{(\nu)\in GT_\mu} 1=|GT_\mu|$$ Therefore we have found that $s_\mu(1^{(B+C)})=|GT_\mu|$
		
		Now, using the result of problem 4, we find that 
		\begin{align*}s_\mu(1^{(B+C)})&=\prod_{1\leq i\leq j\leq B+C}\frac{\mu_i-\mu_j+j-i}{j-i}\\
		&=\prod_{1\leq i\leq j \leq C}\frac{\mu_i-\mu_j+j-i}{j-i}\cdot\prod_{1\leq i \leq C<j\leq B+C} \frac{\mu_i-\mu_j+j-i}{j-i} \cdot \prod_{C\leq i\leq j \leq B+C}\frac{\mu_i-\mu_j+j-i}{j-i}\\
		&=\prod_{1\leq i\leq j \leq C}\frac{A-A+j-i}{j-i}\cdot\prod_{1\leq i \leq C<j\leq B+C} \frac{A-0+j-i}{j-i} \cdot \prod_{C\leq i\leq j \leq B+C}\frac{0-0+j-i}{j-i}\\
		&=\prod_{(i,j)\in E} \frac{A+j-i}{j-i}
		\end{align*} where $E=\llbracket{0,C}\rrbracket \times \llbracket C+1, B+C\rrbracket$
	\subsection*{Problem 11}

\end{document}
