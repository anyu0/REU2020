%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
% Appendix A
%
%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\section{Appendix A} \label{Section8}

\subsection{Proof of Lemma \ref{Polish}}\label{Section8.1}

Observe that the sets $K_1\subset K_2\subset\cdots\subset\Sigma\times\Lambda$ are compact, they cover $\Sigma\times\Lambda$, and any compact subset $K$ of $\Sigma\times\Lambda$ is contained in all $K_n$ for sufficiently large $n$. To see this last fact, let $\pi_1,\pi_2$ denote the canonical projection maps of $\Sigma\times\Lambda$ onto $\Sigma$ and $\Lambda$ respectively. Since these maps are continuous, $\pi_1(K)$ and $\pi_2(K)$ are compact in $\Sigma$ and $\Lambda$. This implies that $\pi_1(K)$ is finite, so it is contained in $\Sigma_{n_1} = \Sigma\cap\llbracket -n_1,n_1\rrbracket$ for some $n_1$. On the other hand, $\pi_2(K)$ is closed and bounded in $\mathbb{R}$, thus contained in some closed interval $[\alpha,\beta]\subseteq\Lambda$. Since $a_n\searrow a$ and $b_n\nearrow b$, we can choose $n_2$ large enough so that $\pi_2(K)\subseteq[\alpha,\beta]\subseteq[a_{n_2},b_{n_2}]$. Then taking $n=\max(n_1,n_2)$, we have $K \subseteq \pi_1(K) \times \pi_2(K) \subseteq \Sigma_n \times [a_n,b_n] = K_n$.

We now split the proof into several steps.\\

\noindent\textbf{Step 1.} In this step, we show that the function $d$ defined in the statement of the lemma is a metric. For each $n$ and $f,g\in C(\Sigma\times\Lambda)$, we define
\[
d_n(f,g) = \sup_{(i,t)\in K_n} |f(i,t)-g(i,t)|,\quad d_n'(f,g) = \min\{d_n(f,g), 1\} 
\]
Then we have
\[
d(f,g) = \sum_{n=1}^\infty 2^{-n} d_n'(f,g).
\]
Clearly each $d_n$ is nonnegative and satisfies the triangle inequality, and it is then easy to see that the same properties hold for $d_n'$. Furthermore, $d_n'\leq 1$, so $d$ is well-defined. Observe that $d$ is nonnegative, and if $f=g$, then each $d_n'(f,g)=0$, so the sum $d(f,g)$ is 0. Conversely, if $f\neq g$, then since the $K_n$ cover $\Sigma\times\Lambda$, we can choose $n$ large enough so that $K_n$ contains an $x$ with $f(x)\neq g(x)$. Then $d_n'(f,g)\neq 0$, and hence $d(f,g)\neq 0$. Lastly, the triangle inequality holds for $d$ since it holds for each $d_n'$.\\

\noindent\textbf{Step 2.} Now we prove that the topology $\tau_d$ on $C(\Sigma\times\Lambda)$ induced by $d$ is the same as the topology of uniform convergence over compacts, which we denote by $\tau_c$. Recall that $\tau_c$ is generated by the basis consisting of sets
\[
B_K(f,\epsilon) = \Big\{g\in C(\Sigma\times\Lambda) : \sup_{(i,t)\in K} |f(i,t) - g(i,t)| < \epsilon \Big\},
\]
for $K\subset\Sigma\times\Lambda$ compact, $f\in C(\Sigma\times\Lambda)$, and $\epsilon>0$, and $\tau_d$ is generated by sets of the form $B^d_\epsilon(f) = \{g:d(f,g) < \epsilon\}$. 

We first show that $\tau_d \subseteq \tau_c$. It suffices to prove that every set $B_\epsilon^d(f)$ is a union of sets $B_K(f,\epsilon)$. First, choose $\epsilon>0$ and $f\in C(\Sigma\times\Lambda)$. Let $g\in B^d_\epsilon(f)$. We will find a basis element $A_g$ of $\tau_c$ such that $g\in A_g\subset B^d_\epsilon(f)$. Let $\delta = d(f,g) < \epsilon$, and choose $n$ large enough so that $\sum_{k>n} 2^{-k} < \frac{\epsilon-\delta}{2}$. Define $A_g = B_{K_n}(g,\frac{\epsilon-\delta}{n})$, and suppose $h\in A_g$. Then since $K_m\subseteq K_n$ for $m\leq n$, we have
\begin{align*}
d(f,h) &\leq d(f,g) + d(g,h) \leq \delta + \sum_{k=1}^n 2^{-k}d_n(g,h) + \sum_{k>n} 2^{-k} \leq \delta + \frac{\epsilon-\delta}{2} + \frac{\epsilon-\delta}{2} = \epsilon.
\end{align*}
Therefore $g\in A_g\subset B^d_\epsilon(f)$. Then we can write
\[
B^d_\epsilon(f) = \bigcup_{g\in B^d_\epsilon(f)} A_g,
\]
a union of basis elements of $\tau_c$.

We now prove conversely that $\tau_c\subseteq\tau_d$. Let $K\subset\Sigma\times\Lambda$ be compact, $f\in C(\Sigma\times\Lambda)$, and $\epsilon>0$. Choose $n$ so that $K\subset K_n$, and let $g\in B_K(f,\epsilon)$ and $\delta = \sup_{x\in K} |f(x)-g(x)| < \epsilon$. If $d(g,h) < 2^{-n}(\epsilon-\delta)$, then $d_n'(g,h) \leq 2^n d(g,h) < \epsilon-\delta$, hence $d_n(g,h) < \epsilon-\delta$, assuming without loss of generality that $\epsilon \leq 1$. It follows that
\begin{align*}
\sup_{x\in K} |f(x)-h(x)| &\leq \delta + \sup_{x\in K} |g(x)-h(x)| \leq \delta + d_n(g,h) \leq \delta + \epsilon-\delta = \epsilon.
\end{align*}
Thus $g\in B^d_{2^{-n}(\epsilon-\delta)}(g) \subset B_K(f,\epsilon)$, proving that $B_K(f,\epsilon)\in\tau_d$ by the same argument as above. We conclude that $\tau_d = \tau_c$.\\

\noindent\textbf{Step 3.} In this step, we show that $(C(\Sigma\times\Lambda), d)$ is a complete metric space. Let $\{f_n\}_{n\geq 1}$ be Cauchy with respect to $d$. Then we claim that $\{f_n\}$ must be Cauchy with respect to $d_n'$, on each $K_n$. This follows from the observation that $ d_n'(f_\ell, f_m) \leq 2^n d(f_\ell, f_m)$. Thus $\{f_n\}$ is Cauchy with respect to the uniform metric on each $K_n$, and hence converges uniformly to a continuous limit $f^{K_n}$ on each $K_n$ (see \cite[Theorem 7.15]{Rudin}). Since the pointwise limit must be unique at each $x\in \Sigma\times\Lambda$, we have $f^{K_n}(x) = f^{K_m}(x)$ if $x\in K_n\cap K_m$. Since $\bigcup K_n = \Sigma\times\Lambda$, we obtain a well-defined function $f$ on all of $\Sigma\times\Lambda$ given by $f(x)=\lim_{n\to\infty} f^{K_n}(x)$. We have $f\in C(\Sigma\times\Lambda)$ since $f|_{K_n} = f^{K_n}$ is continuous on $K_n$ for all $n$. Moreover, if $K\subset\Sigma\times\Lambda$ is compact and $n$ is large enough so that $K\subset K_n$, then because $f_n \to f^{K_n} = f|_{K_n}$ uniformly on $K_n$, we have $f_n \to f^{K_n}|_K = f|_K$ uniformly on $K$. That is, for any $K\subset\Sigma\times\Lambda$ compact and $\epsilon>0$, we have $f_n \in B_K(f,\epsilon)$ for all sufficiently large $n$. Therefore $f_n \to f$ in $\tau_c$, and equivalently in the metric $d$ by Step 2.\\

\noindent\textbf{Step 4.} Lastly, we prove separability, c.f. \cite[Example 1.3]{Billing}. For each pair of positive integers $n,k$, let $D_{n,k}$ be the subcollection of $C(\Sigma\times\Lambda)$ consisting of polygonal functions that are piecewise linear on $\{j\}\times I_{n,k,i}$ for each $j\in\Sigma_n$ and each subinterval 
\[
I_{n,k,i} = \big[a_n+\tfrac{i-1}{k}(b_n-a_n), \, a_n+\tfrac{i}{k}(b_n-a_n)\big], \quad 1\leq i\leq k,
\] 
taking rational values at the endpoints of these subintervals, and extended linearly to all of $\Lambda = [a,b]$. Then $D = \bigcup_{n,k} D_{n,k}$ is countable, and we claim that it is dense in $\tau_c$. To see this, let $K\subset\Sigma\times\Lambda$ be compact, $f\in C(\Sigma\times\Lambda)$, and $\epsilon>0$, and choose $n$ so that $K\subset K_n$. Since $f$ is uniformly continuous on $K_n$, we can choose $k$ large enough so that for $0\leq i\leq k$, if $t\in I_{n,k,i}$, then 
\[
\big|f(j,t) - f(j, a_n + \tfrac{i}{k}(b_n-a_n))\big| < \epsilon/2
\]
for all $j\in\Sigma_n$. We then choose $g\in \bigcup_k D_{n,k}$ with $|g(j,a_n + \frac{i}{k}(b_n-a_n)) - f(j,a_n + \frac{i}{k}(b_n-a_n))| < \epsilon/2$. Then we have 
\begin{align*}
	\big|f(j,t) - g(j, a_n + \tfrac{i-1}{k}(b_n-a_n))\big| < \epsilon \quad \mathrm{and} \quad \big|f(j,t) - g(j, a_n + \tfrac{i}{k}(b_n-a_n))\big| < \epsilon.
\end{align*}
Since $g(j, a_n + \tfrac{i-1}{k}(b_n-a_n)) \leq g(j,t) \leq g(j, a_n + \tfrac{i}{k}(b_n-a_n))$, it follows that 
\[
|f(j,t) - g(j,t)| < \epsilon
\]
as well. In summary,
\[
\sup_{(j,t)\in K} |f(j,t)-g(j,t)| \leq \sup_{(j,t)\in K_n} |f(j,t)-g(j,t)| < \epsilon,
\] 
so $g\in B_K(f,\epsilon)$. This proves that $D$ is a countable dense subset of $C(\Sigma\times\Lambda)$.


\subsection{Proof of Lemma \ref{2Tight}}\label{Section8.2}

We first prove two lemmas that will be used in the proof of Lemma \ref{2Tight}. The first result allows us to identify the space $C(\Sigma\times\Lambda)$ with a product of copies of $C(\Lambda)$. In the following, we assume the notation of Lemma \ref{2Tight}.

\begin{lemma}\label{ProdTop}
	Let $\pi_i: C (\Sigma \times \Lambda) \rightarrow C(\Lambda)$, $i \in \Sigma$, be the projection maps given by
	$\pi_i(F)(x) = F(i, x)$ for $x \in \Lambda$. Then the $\pi_i$ are continuous. Endow the space $\prod_{i\in\Sigma} C(\Lambda)$ with the product topology induced by the topology of uniform convergence over compacts on $C(\Lambda)$. Then the mapping
	\begin{align*}
		F : C(\Sigma\times\Lambda) \longrightarrow \prod_{i\in\Sigma} C(\Lambda), \quad f\mapsto (\pi_i(f))_{i\in\Sigma}
	\end{align*}
	is a homeomorphism.
\end{lemma}

\begin{proof}
	We first prove that the $\pi_i$ are continuous. Since $C(\Sigma\times\Lambda)$ is metrizable by Lemma \ref{Polish}, and by a similar argument so is $C(\Lambda)$, it suffices to assume that $f_n\to f$ in $C(\Sigma\times\Lambda)$ and show that $\pi_i(f_n)\to \pi_i(f)$ in $C(\Lambda)$. Let $K$ be compact in $\Lambda$. Then $\{i\}\times K$ is compact in $\Sigma\times\Lambda$, and $f_n\to f$ on $\{i\}\times K$ by assumption, so we have $\pi_i(f_n)|_K = f_n|_{\{i\}\times K} \to f|_{\{i\}\times K} = \pi_i(f)|_K$ uniformly on $K$. Since $K$ was arbitrary, we conclude that $\pi_i(f_n) \to \pi_i(f)$ in $C(\Lambda)$ as desired. 
	
	We now observe that $F$ is invertible. If $(f_i)_{i\in\Sigma} \in \prod_{i\in\Sigma} C(\Lambda)$, then the function $f$ defined by $f(i,\cdot) = f_i(\cdot)$ is in $C(\Sigma\times\Lambda)$, since $\Sigma$ has the discrete topology. This gives a well-defined inverse for $F$. It suffices to prove that $F$ and $F^{-1}$ are open maps.
	
	We first show that $F$ sends each basis element $B_K(f,\epsilon)$ of $C(\Sigma\times\Lambda)$ to a basis element in $\prod_{i\in\Sigma} C(\Lambda)$. Note that a basis for the product topology is given by products $\prod_{i\in\Sigma} B_{K_i}(f_i,\epsilon)$, where at most finitely many of the $K_i$ are nonempty. Here, we use the convention that $B_{\varnothing}(f_i,\epsilon) = C(\Lambda)$. Let $\pi_\Sigma,\pi_\Lambda$ denote the canonical projections of $\Sigma\times\Lambda$ onto $\Sigma,\Lambda$. The continuity of $\pi_\Sigma$ implies that if $K\subset\Sigma\times\Lambda$ is compact, then $\pi_\Sigma(K)$ is compact in $\Sigma$, hence finite. Observe that the set $K\cap(\{i\}\times\Lambda)$ is an intersection of two compacts sets, hence compact in $\Sigma\times\Lambda$. Therefore the sets $K_i = \pi_\Lambda(K\cap(\{i\}\times\Lambda))$ are compact in $\Lambda$ for each $i\in\Sigma$ since $\pi_\Lambda$ is continuous. We observe that $F(B_K(f,\epsilon)) = \prod_{i\in\Sigma} U_i$, where
	\[
	U_i = B_{K_i}(\pi_i(f),\epsilon), \quad\mathrm{if} \quad i \in \pi_\Sigma(K),
	\]
	and $U_i = C(\Lambda)$ otherwise. Since $\pi_\Sigma(K)$ is finite and the $K_i$ are compact, we see that $F(B_K(f,\epsilon))$ is a basis element in the product topology as claimed.
	
	Lastly, we show that $F^{-1}$ sends each basis element $U = \prod_{i\in\Sigma} B_{K_i}(f_i,\epsilon)$ for the product topology to a set of the form $B_K(f,\epsilon)$. We have $K_i=\varnothing$ for all but finitely many $i$. Write $f = F^{-1}((f_i)_{i\in\Sigma})$ and $K=\prod_{i\in\Sigma} K_i$. By Tychonoff's theorem, \cite[Theorem 37.3]{Munkres}, $K$ is compact in $\Sigma\times\Lambda$, and
	\[
	F^{-1}(U) = B_K(f,\epsilon).
	\]
	
\end{proof}

We next prove a lemma which states that a sequence of line ensembles is tight if and only if all individual curves form tight sequences.

\begin{lemma}\label{ProjTight}
	Suppose that $\{\mathcal{L}^n\}_{n\geq 1}$ is a sequence of $\Sigma$-indexed line ensembles on $\Lambda$, and let $X_i^n = \pi_i(\mathcal{L}^n)$. Then the $X_i^n$ are $C(\Lambda)$-valued random variables on $(\Omega,\mathcal{F},\mathbb{P})$, and $\{\mathcal{L}^n\}$ is tight if and only if for each $i \in \Sigma$ the sequence $\{X_i^n\}_{n\geq 1}$ is tight.
	
\end{lemma}

\begin{proof}

The fact that the $X_i^n$ are random variables follows from the continuity of the $\pi_i$ in Lemma \ref{ProdTop} and \cite[Theorem 1.3.5]{Durrett}. First suppose the sequence $\{\mathcal{L}^n\}$ is tight. By Lemma \ref{Polish}, $C(\Sigma\times\Lambda)$ is a Polish space, so it follows from Prohorov's theorem, \cite[Theorem 5.1]{Billing}, that $\{\mathcal{L}^n\}$ is relatively compact. That is, every subsequence $\{\mathcal{L}^{n_k}\}$ has a further subsequence $\{\mathcal{L}^{n_{k_\ell}}\}$ converging weakly to some $\mathcal{L}$. Then for each $i\in\Sigma$, since $\pi_i$ is continuous by the above, the subsequence $\{\pi_i(\mathcal{L}^{n_{k_\ell}})\}$ of $\{\pi_i(\mathcal{L}^{n_k})\}$ converges weakly to $\pi_i(\mathcal{L})$ by the continuous mapping theorem, \cite[Theorem 3.2.10]{Durrett}. Thus every subsequence of $\{\pi_i(\mathcal{L}^n)\}$ has a convergent subsequence. Since $C(\Lambda)$ is a Polish space by the same argument as in the proof of Lemma \ref{Polish}, Prohorov's theorem implies that each $\{\pi_i(\mathcal{L}^n)\}$ is tight.

Conversely, suppose $\{X_i^n\}$ is tight for all $i\in\Sigma$. Then given $\epsilon > 0$, we can find compact sets $K_i\subset C(\Lambda)$ such that
\[
\mathbb{P}(X_i^n \notin K_i) \leq \epsilon/2^i
\]
for each $i\in\Sigma$. By Tychonoff's theorem, \cite[Theorem 37.3]{Munkres}, the product $\tilde{K} = \prod_{i\in\Sigma} K_i$ is compact in $\prod_{i\in\Sigma} C(\Lambda)$. We have
\begin{equation}\label{tychonoff}
\mathbb{P}\big((X_i^n)_{i\in\Sigma} \notin \tilde{K} \big) \leq \sum_{i\in\Sigma} \mathbb{P}(X_i^n \notin K_i) \leq \sum_{i=1}^\infty \epsilon/2^i = \epsilon.
\end{equation}
By Lemma \ref{ProdTop}, we have a homeomorphism $G : \prod_{i\in\Sigma} C(\Lambda) \to C(\Sigma\times\Lambda)$. We observe that $G((X_i^n)_{i\in\Sigma}) = \mathcal{L}^n$, and $K = G(\tilde{K})$ is compact in $C(\Sigma\times\Lambda)$. Thus $\mathcal{L}^n \in K$ if and only if $(X_i^n)_{i\in\Sigma} \in \tilde{K}$, and it follows from \eqref{tychonoff} that
\[
\mathbb{P}(\mathcal{L}^n \in K) \geq 1 - \epsilon.
\]
This proves that $\{\mathcal{L}^n\}$ is tight.

\end{proof}

We are now ready to prove Lemma \ref{2Tight}.

\begin{proof}
	
Fix an $i\in\Sigma$. By Lemma \ref{ProjTight}, it suffices to show that the sequence $\{\mathcal{L}_i^n\}_{n\geq 1}$ of $C(\Lambda)$-valued random variables is tight. By \cite[Theorem 7.3]{Billing}, a sequence $\{P_n\}$ of probability measures on $C[0,1]$ with the uniform topology is tight if and only if the following conditions hold:
\begin{align*}
\lim_{a\to\infty} \limsup_{n\to\infty} P_n(|x(0)|\geq a) &= 0, \\
\lim_{\delta\to 0} \limsup_{n\to\infty} P_n\Big(\sup_{|s-t|\leq\delta} |x(s)-x(t)| \geq \epsilon\Big) &= 0 \quad \textrm{for all}\;\epsilon>0.
\end{align*}
By replacing $[0,1]$ with $[a_m,b_m]$ and 0 with $a_0$, we see that the hypotheses in the lemma imply that the sequence $\{\mathcal{L}_i^n|_{[a_m,b_m]}\}_n$ is tight for every $m\geq 1$. Let $\pi_m : C(\Lambda) \to C([a_m,b_m])$ denote the map $f \mapsto f|_{[a_m,b_m]}$. Then $\pi_m$ is continuous, since $C(\Lambda)$ and $C([a_m,b_m])$ with the topologies of uniform convergence over compacts are metrizable by Lemma \ref{Polish}, and if $f_n\to f$ uniformly on compact subsets of $\Lambda$, then $f_n|_{[a_m,b_m]} \to f|_{[a_m,b_m]}$ uniformly on compact subsets of $[a_m,b_m]$. It follows from \cite[Theorem 1.3.5]{Durrett} that $\pi_m(\mathcal{L}^n) = \mathcal{L}_i^n|_{[a_m,b_m]}$ is a $C([a_m,b_m])$-valued random variable. Tightness of the sequence implies that for any $\epsilon > 0$, we can find compact sets $K_m\subset C([a_m,b_m])$ so that
\[
\mathbb{P}\big(\pi_m(\mathcal{L}_i^n) \notin K_m \big) \leq \epsilon/2^m
\]
for each $m\geq 1$. Writing $K = \bigcap_{m=1}^\infty \pi_m^{-1}(K_m)$, it follows that
\[
\mathbb{P}\big(\mathcal{L}^n_i \in K\big) \geq 1 - \sum_{m=1}^\infty \epsilon/2^m = 1 - \epsilon.
\]
To conclude tightness of $\{\mathcal{L}_i^n\}$, it suffices to prove that $K = \bigcap_{m=1}^\infty \pi_m^{-1}(K_m)$ is sequentially compact in $C(\Lambda)$. We argue by diagonalization. Let $\{f_n\}$ be a sequence in $K$, so that $f_n|_{[a_m,b_m]} \in K_m$ for every $m,n$. Since $K_1$ is compact, there is a sequence $\{n_{1,k}\}$ of natural numbers such that the subsequence $\{f_{n_{1,k}}|_{[a_1,b_1]}\}_k$ converges in $C([a_1,b_1])$. Since $K_2$ is compact, we can take a further subsequence $\{n_{2,k}\}$ of $\{n_{1,k}\}$ so that $\{f_{n_{2,k}}|_{[a_2,b_2]}\}_k$ converges in $C([a_2,b_2])$. Continuing in this manner, we obtain sequences $\{n_{1,k}\} \supseteq \{n_{2,k}\} \supseteq\cdots$ so that $\{f_{n_{m,k}}|_{[a_m,b_m]}\}_k$ converges in $C([a_m,b_m])$ for all $m$. Writing $n_k = n_{k,k}$, it follows that the sequence $\{f_{n_k}\}$ converges uniformly on each $[a_m,b_m]$. If $K$ is any compact subset of $C(\Lambda)$, then $K\subset [a_m,b_m]$ for some $m$, and hence $\{f_{n_k}\}$ converges uniformly on $K$. Therefore $\{f_{n_k}\}$ is a convergent subsequence of $\{f_n\}$.

\end{proof}
\subsection{Proof of Lemma \ref{LemmaWD}}\label{LemmaWDProof}
	\begin{proof}
		We will construct a candidate $\mathfrak{B}$ of $\Omega_{avoid}(T_0,T_1,\vec x,\vec y, f,g)$ with the conditions of \ref{LemmaWD} assumed. Construct the ensemble $\mathfrak{B}$ in the following manner. Denote $B_0=f$ and $B_{k+1}=g$ with $x_0=f(T_0)$ and $y_0=f(T_1)$. By Condition (3) of Lemma \ref{LemmaWD} we know $x_0\geq x_1$ and $y_0\geq y_1$. Then let $B_j(T_0)=x_j$ for all $j\in \llbracket 1,k\rrbracket$ and then for all $i\in \llbracket T_0, T_{1}-1\rrbracket$ we have \begin{equation}\label{MaxEns}
		B_j(i+1)=\begin{cases}
		B_j(i)+1 & \text{if } B_j(i)+1\leq \min\{B_{j-1}(i+1),y_j\}\\
		B_j(i) &\text{Else.}
		\end{cases}
		\end{equation} This definition is well-defined, since we may find $B_1$ depending solely on the predetermined $f$, and then inductively find $B_j$ since $B_{j-1}$ has been determined by the previous curves in $\mathfrak{B}$.
		
		In order to verify that this candidate ensemble $\mathfrak{B}$ is an element of $\Omega_{avoid}(T_0,T_1,\vec x,\vec y,f,g)$, three properties must be ensured:
		\begin{equation}\label{nonEmpCond}
		\begin{split}
		\text{(a) } &\mathfrak{B}(T_0)=\vec x\text{ and }\mathfrak{B}(T_1)=\vec y\\
		\text{(b) } &f(i)\geq B_1(i)\geq \cdots \geq B_k(i)\geq g(i)\text{ for all }i\in \llbracket T_0,T_1\rrbracket\\
		\text{(c) } &B_{j}(i+1)-B_j(i)\in \{0,1\}\text{ for all }i\in \llbracket T_0,T_1-1\rrbracket\text{ and }j\in \llbracket 1,k\rrbracket
		\end{split}
		\end{equation} Property (c) follows directly from Definition \ref{MaxEns}, since $B_{j}(i+1)=B_j(i)$ or $B_{j}(i+1)=B_j(i)+1$, and hence $B_j(i+1)-B_j(i)\in \{0,1\}$. The remainder of the proof will be broken up into two steps, the first step proving property (a), and the second proving Property (b).\\\\
		\noindent\textbf{Step 1:}\\
		We know by definition that $\mathfrak{B}(T_0)=\vec x$, and we claim that $\mathfrak{B}(T_1)=\vec y$. We will show this claim inductively on $j$: We trivially know the claim is true for $j=0$, since $y_0=f(T_1)$ is given. Then suppose that $B_j(T_1)=x_j$ holds upto $j=n-1$. First, we know by definition that $B_{n}(i+1)=B_{n}(i)$ if either $B_{n}(i)=y_{n}$ or $B_{n}(i)+1>B_{n}(i+1)$.  Suppose that for some $i_0\in \llbracket{T_0,T_1}\rrbracket$ we have $B_n(i_0)=B_n(i_0+1)$.
		
		If $B_{n}(i_0)=B_{n}(i_0+1)$ because $B_{n}(i_0)=y_n$, then $B_n(T_1)=y_n$ since 
		\begin{equation*}y_n=B_n(i_0)=B_n(i_0+1)=\dots=B_n(T_1)\end{equation*}
		and then the claim is true, namely that $B_n(T_1)=y_n$, and so induction holds.
		
		Then, for the other case, when $B_n(i_0)=B_{n}(i_0+1)$ because $B_{n}(i_0)+1>B_{n-1}(i_0+1)$, we first need to prove that $B_j(i)\leq B_{j-1}(i)$ for $j\in \llbracket 1,k\rrbracket$ for any $i\in \llbracket T_0,T_1\rrbracket$. We know this is true for $T_0$ since $x_0\geq x_1\geq \cdots \geq x_k$. Then, inductively we know that if $B_j(i)\leq B_{j-1}(i)$, $B_j(i+1)=B_j(i)$ or $B_j(i)+1$. In the first case, $B_j(i+1)=B_j(i)\leq B_{j-1}(i)\leq B_j(i)$ by property (3) of \ref{nonEmpCond}. Then, if $B_j(i+1)=B_j(i)+1$ implies $B_j(i+1)\leq B_j(i+1)$ by equation \ref{MaxEns}. Hence, we know that for $i\in \llbracket T_0,T_1\rrbracket$\begin{equation}\label{NonEmpPartialIneq}
		f(i)\geq B_1(i)\geq \cdots \geq B_k(i)
		\end{equation}
		Therefore, we know that $B_n(i_0)=B_n(i_0+1)$ and $B_n(i_0)+1>B_{n-1}(i_0+1)$, which implies $B_n(i)=B_{n-1}(i)$. This implies that if we denote $i_1$ as the least $i$ such that $B_{n-1}(i_1)=y_n$ then \begin{equation*}
		B_n(i)=B_{n-1}(i)\text{ for all }i\in \llbracket i_0, i_1\rrbracket
		\end{equation*} We know that there exists such an $i_1\in \llbracket{T_0,T_1}\rrbracket$ because where $i_1$ is the first $i$ such that $B_{n-1}(i_1)=y_n$, since if $B_{n-1}(i+1)=B_{n-1}(i)$ then $B_n(i)+1=B_{n-1}(i)+1>B_{n-1}(i+1)$ by $\ref{MaxEns}$, the definition of $\mathfrak{B}$. Therefore we know $B_n(i+1)=B_n(i)=B_{n-1}(i)=B_{n-1}(i+1)$.
		
		If $B_{n-1}(i+1)=B_n(i)+1$ then $B_n(i)+1\leq B_{n-1}(i+1)$ by \ref{MaxEns} so $B_n(i+1)=B_{n-1}(i+1)$ therefore inductively until $B_n$ cannot increase above $y_n$, we know $B_n(i)=B_{n-1}(i)$. Because we know that there is some $i_1$ such that $B_{n-1}(i_1)=y_n$, and hence $B_n(i_1)=y_n$ we get $B_n(T_1)=y_n$ and the claim that $B_n(T_1)=y_n$ is true if there exists some $i_0$ such that $B_n(i_0)=B_n(i_0+1)$
		
		Finally, assume that there exists no such $i_0$ that $B_n(i_0)=B_n(i_0+1)$. Then conversely $B_{n}(i)+1\leq B_{n}(i+1)$ for all $i$, then we know that $B_{n}(i+1)=B_{n}(i)+1$ for all $i$ unless $B_{n}(i)=y_{n}$ by \ref{MaxEns}. Therefore, until $B_{n}(i)=y_{n}$, we have $B_{n}(i+s)=B_{n}(i)+s$ hence $B_{n}(T_0+y_{n}-x_{n})=B_{n}(T_0)+y_{n}-x_{n}=y_{n}$. By the inequality in condition (1) of \ref{LemmaWD}, we have the following inequalities:
		\begin{equation}\begin{split}
		&T_1-T_0\geq y_{n}-x_{n}\geq 0 \\
		&T_0\leq T_0+y_{n}-x_{n}\\
		&T_1\geq T_0+y_{n}-x_{n}\end{split}\end{equation} so $T_0+y_{n}-x_{n}\in \llbracket T_0,T_1\rrbracket$ and so $B_n(T_0+y_{n}-x_n)B_{n}(T_1)=y_{n}$. This means whether or not $i_0$ exists, the induction holds and therefore we know that for all $j$ we have $B_j(T_1)=y_j$, so we know that $\mathfrak{B}(T_0)=\vec x$ and $\mathfrak{B}(T_1)=\vec y$ which concludes Step 1, proving Property (a) of \ref{nonEmpCond}.
		
		\textbf{Step 2:} Now all that is left to verify avoidance, or Property (b) of $\ref{nonEmpCond}$. In equation \ref{NonEmpPartialIneq}, we already found that 
		\begin{equation*}
		f(i)\geq B_1(i)\geq \cdots \geq B_k(i)
		\end{equation*} so we must only prove that $B_{k}(i)\geq g(i)$ for all $i$. Suppose that $g(i)>B_k(i)$ for some $i\in \llbracket T_0,T_1\rrbracket$. Since $g(T_0)<B_k(T_0)=x_k$ by Condition (3) in Lemma $\ref{LemmaWD}$, we know that there exists some point $i_0$ such that $g(i_0)=B_k(i_0)$ and $g(i_0+1)>B_k(i_0+1)$. In particular, since $g$ and $B_k$ can each only increase by $1$, this implies $B_k(i_0)=B_k(i_0+1)$. This implies either $B_k(i_0)=y_k$ or $B_k(i_0)+1>B_{k-1}(i_0+1)$. If $B_k(i_0)=y_k$ then since $g(i_0+1)\leq y_k$ by Condition (3) of Lemma \ref{LemmaWD}, there is a contradiction. 
		
		Therefore, it must be the case that $B_k(i)+1>B_{k-1}(i+1)$. Then we find that for any $j\in \llbracket 1,k\rrbracket$ we know that $B_j(i)+1>B_{j-1}(i+1)$ implies $B_j(i_0)=B_{j-1}(i_0)$ since $B_j(i_0)=B_j(i_0+1)$ and $B_{j-1}(i_0)\geq B_j(i_0)$ . This can be applied to each $j$ to find that $g(i_0)=f(i_0)$ and $g(i_0+1)>f(i_0+1)$, which we assumed not to be the case. Therefore, we know that $g\leq B_k$ and so we have proven property (3), implying that if the three conditions in the statement of Lemma \ref{LemmaWD} are met then we know $\mathfrak{B}\in \Omega_{avoid}(T_0,T_1,\vec x,\vec y, f,g)$ and so $\Omega_{avoid}(T_0,T_1,\vec x,\vec y, f,g)$ is non-empty.
	\end{proof}

\subsection{Proof of Lemmas \ref{scaledavoidBB} and \ref{inftydistinct}}\label{BGPapp}

We first prove Lemma \ref{scaledavoidBB}. We will use the following lemma, which proves an analogous convergence result for a single rescaled Bernoulli random walk.

\begin{lemma}\label{scaledRWbb}
	Let $x,y,a,b\in \mathbb{R}$ with $a<b$, and let $a_N,b_N\in N^{-\alpha}\mathbb{Z}$, $x^N,y^N \in N^{-\alpha/2}\mathbb{Z}$ be sequences with $a_N \leq a$, $b_N\geq b$, and $|y^N-x^N| \leq (b_N-a_N)N^{\alpha/2}$. Suppose $a_N\to a$, $b_N\to b$. Write $\tilde{x}^N = (x^N - pa_NN^{\alpha/2})/\sqrt{p(1-p)}$, $\tilde{y}^N = (y^N - pb_NN^{\alpha/2})/\sqrt{p(1-p)}$, and assume $\tilde{x}^N \to x$, $\tilde{y}^N\to y$ as $N\to\infty$. Let $Y^N$ be a sequence of random variables with laws $\mathbb{P}^{a_N,b_N,x^N,y^N}_{free,N}$, and let $Z^N = Y^N|_{[a,b]}$. Then the law of $Z^N$ converges weakly to $\mathbb{P}^{a,b,x,y}_{free}$ as $N\to\infty$.
\end{lemma}

\begin{proof}
	Let us write $z^N = (y^N-x^N)N^{\alpha/2}$ and $T_N = (b_N-a_N)N^\alpha$. Let $\tilde{B}$ be a standard Brownian bridge on $[0,1]$, and define random variables $B^N$, $B$ taking values in $C([a_N,b_N])$, $C([a,b])$ respectively via
	\begin{align*}
	B^N(t) &= \sqrt{b_N-a_N}\cdot\tilde{B}\left(\frac{t-a_N}{b_N-a_N}\right) + \frac{t-a_N}{b_N-a_N}\cdot \tilde{y}^N + \frac{b_N-t}{b_N-a_N}\cdot \tilde{x}^N,\\
	B(t) &= \sqrt{b-a}\cdot \tilde{B}\left(\frac{t-a}{b-a}\right) + \frac{t-a}{b-a}\cdot y + \frac{b-t}{b-a}\cdot x.
	\end{align*}
	We observe that $B$ has law $\mathbb{P}^{a,b,x,y}_{free}$ and $B^N\implies B$ as $N\to\infty$. By \cite[Theorem 3.1]{Billing}, to show that $Z^N\implies B$, it suffices to find a sequence of probability spaces supporting $Y^N,B^N$ so that
	\begin{equation}\label{scaledRWweak}
	\rho(B^N,Y^N) = \sup_{t\in[a_N,b_N]} |B^N(t) - Y^N(t)| \implies 0 \quad \mathrm{as} \quad N\to\infty.
	\end{equation}
	It follows from Theorem \ref{KMT} that for each $N\in\mathbb{N}$ there is a probability space supporting $B^N$ and $Y^N$, as well as constants $C,a',\alpha' > 0$, such that
	\begin{equation}\label{scaledRWcheb}
	\ex\left[e^{a'\Delta(N,x^N,y^N)}\right] \leq Ce^{\alpha'\log N} e^{|z^N-pT_N|^2/N^\alpha},
	\end{equation}
	where $\Delta(N,x^N,y^N) = \sqrt{p(1-p)}\,N^{\alpha/2}\rho(B^N,Y^N)$. Since $(z^N - pT_N)N^{-\alpha/2} \to \sqrt{p(1-p)}\,(y-x)$ by assumption, there exist $N_0\in\mathbb{N}$ and $A>0$ so that $|z-pT_N| \leq AN^{\alpha/2}$ for $N\geq N_0$. Then for $\epsilon > 0$ and $N\geq N_0$, Chebyshev's inequality and \eqref{scaledRWcheb} give
	\[
	\mathbb{P}(\rho(B^N,Y^N) > \epsilon) \leq C e^{-a'\epsilon\sqrt{p(1-p)}\, N^{\alpha/2}}e^{\alpha'\log N} e^{A^2}.
	\]
	The right hand side tends to 0 as $N\to\infty$, implying \eqref{scaledRWweak}.
\end{proof}

We now give the proof of Lemma \ref{scaledavoidBB}.

\begin{proof}
	We prove the two statements of the lemma in two steps.\\
	
	\noindent\textbf{Step 1. } In this step we fix $N_0 \in \mathbb{N}$ so that $\mathbb{P}^{a_N,b_N,\vec{x}\,^N,\vec{y}\,^N,f_N,g_N}_{avoid,N}$ is well-defined for $N\geq N_0$. Observe that we can choose $\epsilon > 0$ and continuous functions $h_1,\dots,h_k : [a,b]\to\mathbb{R}$ depending on $a,b,\vec{x},\vec{y},f,g$ with $h_i(a) = x_i$, $h_i(b)=y_i$ for $i\in\llbracket 1,k\rrbracket$, such that if $u_i:[a,b]\to\mathbb{R}$ are continuous functions with $\rho(u_i,h_i) = \sup_{x\in[a,b]} |u_i(x)-h_i(x)| < \epsilon$, then
	\begin{equation}\label{scaledavoidwindow}
	f(x) - \epsilon > u_1(x) + \epsilon > u_1(x) - \epsilon > \cdots > u_k(x) + \epsilon > u_k(x) - \epsilon > g(x) + \epsilon
	\end{equation}
	for all $x\in[a,b]$. By Lemma \ref{Spread}, we have
	\begin{equation}\label{BBwindow}
	\mathbb{P}^{a,b,\vec{x},\vec{y}}_{free}(\rho(\mathcal{Q}_i,h_i) > \epsilon \mbox{ for } i\in\llbracket 1,k\rrbracket) > 0.
	\end{equation}
	Since $y_i^N - x_i^N - p(b_N-a_N)N^{\alpha/2} \to \sqrt{p(1-p)}\,(y_i-x_i)$ as $N\to\infty$ for $i\in\llbracket 1,k\rrbracket$ and $p<1$, we can find $N_1\in\mathbb{N}$ so that for $N\geq N_1$, $|y_i^N-x_i^N| \leq (b_N-a_N)N^{\alpha/2}$. It follows from Lemma \ref{scaledRWbb} that if $\mathcal{Y}^N$ have laws $\mathbb{P}^{a_N,b_N,\vec{x}\,^N,\vec{y}\,^N}_{free,N}$ for $N\geq N_1$ and $\mathcal{Z}^N = \mathcal{Y}^N|_{\Sigma\times[a,b]}$, then the law of $\mathcal{Z}^N$ converges weakly to $\mathbb{P}^{a,b,\vec{x},\vec{y}}_{avoid}$. In view of \eqref{BBwindow} we can then find $N_2$ so that if $N \geq \max(N_1,N_2)$ then
	\[
	\mathbb{P}^{a_N,b_N,\vec{x}\,^N,\vec{y}\,^N}_{free,N}(\rho(\mathcal{Q}_i,h_i) > \epsilon \mbox{ for } i\in\llbracket 1,k\rrbracket) > 0.
	\]
	We now choose $N_3$ so that $\sup_{x\in[a-1,b+1]}|f(x)-f_N(x)| < \epsilon/4$ and $\sup_{x\in[a-1,b+1]}|g(x)-g_N(x)| < \epsilon/4$. If $f=\infty$ (resp. $g=-\infty$), we interpret this to mean that $f_N=\infty$ (resp. $g_N=-\infty$). We take $N_4$ large enough so that if $N\geq N_4$ and $|x-y|\leq N^{-\alpha/2}$ then $|f(x)-f(y)|<\epsilon/4$ and $|g(x)-g(y)|<\epsilon/4$. Lastly, we choose $N_5$ so that $N_5^{-\alpha} < \epsilon/4$. Then for $N\geq N_0 = \max(N_1,N_2,N_3,N_4,N_5)$, we have
	\[
	\{\rho(\mathcal{Q}_i,h_i) > \epsilon \mbox{ for } i\in\llbracket 1,k\rrbracket\} \subset \{f_N \geq \mathcal{Y}^N_1 \geq \cdots \geq \mathcal{Y}^N_k \geq g_N \mbox{ on } [a_N,b_N]\}.
	\]
	By \eqref{BBwindow}, this implies that $\mathbb{P}^{a_N,b_N,\vec{x}\,^N,\vec{y}\,^N,f_N,g_N}_{avoid,N}$ is well-defined.\\
	
	\noindent\textbf{Step 2. } In this step we prove that $\mathcal{Z}^N \implies \mathbb{P}^{a,b,\vec{x},\vec{y},f,g}_{avoid}$, with $\mathcal{Z}^N$ defined in the statement of the lemma. We write $\Sigma = \llbracket 1,k\rrbracket$, $\Lambda = [a,b]$, and $\Lambda_N = [a_N,b_N]$. It suffices to show that for any bounded continuous function $F : C(\Sigma\times\Lambda)\to\mathbb{R}$ we have
	\begin{equation}\label{scaledavoidweak}
	\lim_{N\to\infty} \ex[F(\mathcal{Z}^N)] = \ex[F(\mathcal{Q})],
	\end{equation}
	where $\mathcal{Q}$ has law $\mathbb{P}^{a,b,\vec{x},\vec{y},f,g}_{avoid}$.
	
	We define the functions $H_{f,g} : C(\Sigma\times\Lambda)\to\mathbb{R}$ and $H^N_{f,g}:C(\Sigma\times\Lambda_N)\to\mathbb{R}$ by
	\begin{align*}
	H_{f,g}(\mathcal{L}) &= \mathbf{1}\{f > \mathcal{L}_1 > \cdots > \mathcal{L}_k > g \mbox{ on } \Lambda\},\\
	H^N_{f,g}(\mathcal{L}^N) &= \mathbf{1}\{f \geq \mathcal{L}^N_1 \geq \cdots \geq \mathcal{L}^N_k \geq g \mbox{ on } \Lambda_N\}.
	\end{align*}
	Then we observe that for $N\geq N_0$,
	\begin{equation}\label{scaledavoidcond}
	\ex[F(\mathcal{Z}^N)] = \frac{\ex[F(\mathcal{L}^N|_{\Sigma\times[a,b]})H_{f,g}^N(\mathcal{L}^N)]}{\ex[H_{f,g}^N(\mathcal{L}^N)]},
	\end{equation}
	where $\mathcal{L}^N$ has law $\mathbb{P}^{a_N,b_N,\vec{x}\,^N,\vec{y}\,^N}_{free,N}$. By our choice of $N_0$ in Step 1, the denominator in \eqref{scaledavoidcond} is positive for all $N\geq N_0$. Similarly, we have
	\begin{equation}\label{BBavoidcond}
	\ex[F(\mathcal{Q})] = \frac{\ex[F(\mathcal{L})H_{f,g}(\mathcal{L})]}{\ex[H_{f,g}(\mathcal{L})]}, 
	\end{equation}
	where $\mathcal{L}$ has law $\mathbb{P}^{a,b,\vec{x},\vec{y}}_{free}$. From \eqref{scaledavoidcond} and \eqref{BBavoidcond}, we see that to prove \eqref{scaledavoidweak} it suffices to show that for any bounded continuous function $F:C(\Sigma\times\Lambda)\to\mathbb{R}$,
	\begin{equation}\label{scaledBBex}
	\lim_{N\to\infty}\ex[F(\mathcal{L}^N|_{\Sigma\times[a,b]})H_{f,g}^N(\mathcal{L}^N)] = \ex[F(\mathcal{L})H_{f,g}(\mathcal{L})].
	\end{equation}
	By Lemma \ref{scaledRWbb}, $\mathcal{L}^N|_{\Sigma\times[a,b]} \implies \mathcal{L}$ as $N\to\infty$. Since $C(\Sigma\times\Lambda)$ is separable, the Skorohod representation theorem \cite[Theorem 6.7]{Billing} gives a probability space $(\Omega,\mathcal{F},\mathbb{P})$ supporting $C(\Sigma\times\Lambda_N)$-valued random variables $\mathcal{L}^N$ with laws $\mathbb{P}^{a_N,b_N,\vec{x}\,^N,\vec{y}\,^N}_{free,N}$ and a $C(\Sigma\times\Lambda)$-valued random variable $\mathcal{L}$ with law $\mathbb{P}^{a,b,\vec{x},\vec{y}}_{free}$ such that $\mathcal{L}^N|_{\Sigma\times[a,b]}\to\mathcal{L}$ uniformly on compact sets, pointwise on $\Omega$. Here we rely on the fact that $a_N,b_N$ are respectively the largest element of $N^{-\alpha}\mathbb{Z}$ less than $a$ and the smallest element greater than $b$, so that $\mathcal{L}^N|_{\Sigma\times[a,b]}$ uniquely determines $\mathcal{L}^N$ on $[a_N,b_N]$.
	
	Define the events
	\begin{align*}
	E_1 &= \{\omega : f > \mathcal{L}_1(\omega) > \cdots > \mathcal{L}_k(\omega) > g \mbox{ on } [a,b]\},\\
	E_2 &= \{\omega : \mathcal{L}_i(\omega)(r) < \mathcal{L}_{i+1}(\omega)(r) \mbox{ for some } i\in\llbracket 0,k\rrbracket \mbox{ and } r\in[a,b]\},
	\end{align*}
	where in the definition of $E_2$ we use the convention $\mathcal{L}_0 = f$, $\mathcal{L}_{k+1} = g$. The continuity of $F$ implies that $F(\mathcal{L}^N|_{\Sigma\times[a,b]})H^N_{f_N,g_N}(\mathcal{L}^N) \to F(\mathcal{L})$ on the event $E_1$, and $F(\mathcal{L}^N|_{\Sigma\times[a,b]})H^N_{f_N,g_N}(\mathcal{L}^N)\to 0$ on the event $E_2$. By Lemma \ref{NoTouch} we have $\mathbb{P}(E_1 \cup E_2) = 1$, so $\mathbb{P}$-a.s. we have $F(\mathcal{L}^N|_{\Sigma\times[a,b]})H^N_{f_N,g_N}(\mathcal{L}^N) \to F(\mathcal{L})H_{f,g}(\mathcal{L})$. The bounded convergence theorem then implies \eqref{scaledBBex}, completing the proof of \eqref{scaledavoidweak}.
\end{proof}

We now state two lemmas about Brownian bridges which will be used in the proof of Lemma \ref{inftydistinct}. The first lemma shows that a Brownian bridge started at 0 almost surely becomes negative somewhere on its domain.

\begin{lemma}\label{BBcross0}
	Fix any $T>0$ and $y\in\mathbb{R}$, and let $Q$ denote a random variable with law $\mathbb{P}^{0,T,0,y}_{free}$. Define the event $C = \{\inf_{s\in[0,T]} Q(s) < 0\}$. Then $\mathbb{P}^{0,T,0,y}_{free}(C) = 1$.
\end{lemma}

\begin{proof}
	Let $B$ denote a standard Brownian bridge on $[0,1]$, and let 
	\[
	\tilde{B}_s = B_{s/T} + \frac{sy}{T}, \quad \mathrm{for}\,s\in[0,T].
	\]
	Then $\tilde{B}$ has the law of $Q$. Consider the stopping time $\tau = \inf\{s>0 : \tilde{B}_s < 0 \}$. We will argue that $\tau = 0$ a.s, which implies the conclusion of the lemma since $\{\tau = 0\} \subset C$. We observe that since $\tilde{B}$ is a.s. continuous and $\mathbb{Q}$ is dense in $\mathbb{R}$,
	\[
	\{\tau = 0\} = \bigcap_{\epsilon > 0}\,\bigcup_{s\in(0,\epsilon)\cap\mathbb{Q}} \{\tilde{B}_s < 0 \} \in \bigcap_{\epsilon > 0} \sigma(\tilde{B}_s : s<\epsilon).
	\]
	Here, $\sigma(\tilde{B}_s : s<\epsilon)$ denotes the $\sigma$-algebra generated by $\tilde{B}_s$ for $s<\epsilon$. We used the fact that for a fixed $\epsilon$, each set $\{\tilde{B}_s < 0\}$ for $s\in (0,\epsilon)\cap\mathbb{Q}$ is contained in this $\sigma$-algebra, and thus so is their countable union. It follows from Blumenthal's 0-1 law \cite[Theorem 7.2.3]{Durrett} that $\mathbb{P}(\tau = 0) \in \{0,1\}$. To complete the proof, it suffices to show that $\mathbb{P}(\tau = 0) > 0$. By \eqref{BBcovar}, $B_{s/T}$ is distributed normally with mean 0 and variance $\sigma^2 = (s/T)(1-s/T)$. We observe that for any $s\in(0,T)$, 
	\[
	\mathbb{P}(\tau\leq s) \geq \mathbb{P}(B_{s/T} < -sy/T) = \mathbb{P}\left(\sigma\mathcal{N}(0,1) > (s/T)y\right) = \mathbb{P}\left(\mathcal{N}(0,1) > y\sqrt{s/(T-s)}\right).
	\]
	As $s\to 0$, the probability on the right tends to $\mathbb{P}(\mathcal{N}(0,1) > 0) = 1/2$. Since $\{\tau = 0\} = \bigcap_{n=1}^\infty \{\tau \leq 1/n\}$ and $\{\tau\leq 1/(n+1)\} \subset \{\tau \leq 1/n\}$, we conclude that
	\[
	\mathbb{P}(\tau = 0) = \lim_{n\to\infty} \mathbb{P}(\tau \leq 1/n) \geq 1/2.
	\]
	Therefore $\mathbb{P}(\tau = 0) = 1$.
	
\end{proof}	

The second lemma shows that a difference of two independent Brownian bridges is another Brownian bridge.

\begin{lemma}\label{BBDif}
	Let $a,b,x_1,y_1,x_2,y_2,\in \R$ with $a<b$. Let $B_1(t)$, $B_2(t)$ be independent Brownian bridges from on $[a,b]$ from $x_1$ to $y_1$ and from $x_2$ to $y_2$ respectively, as defined in \ref{BBDef}. If $B(t)=B_1(t)-B_2(t)$ for $t\in[a,b]$, then $B$ is itself a Brownian bridge on $[a,b]$.
\end{lemma}
\begin{proof}
	By definition, for $i = 1,2$ we have
	\begin{equation*}
		B_i(t)=(b-a)^{1/2}\cdot \tilde{B_i}\left(\frac{t-a}{b-a}\right)+\left(\frac{b-t}{b-a}\right)\cdot x_i+\left(\frac{t-a}{b-a}\right)\cdot y_i,
	\end{equation*} 
	with $\tilde{B_i}(t)=W^i_t-tW^i_1$ for independent Brownian motions $W^1$ and $W^2$. We have
	\begin{equation}\label{BBdifeq}
		B_1(t)-B_2(t)=(b-a)^{1/2}\cdot(\tilde{B_1}-\tilde{B_2})\left(\frac{t-a}{b-a}\right)+\left(\frac{b-t}{b-a}\right)\cdot (x_1-x_2)+\left(\frac{t-a}{b-a}\right)\cdot (y_1-y_2).
	\end{equation}
	Note that the process $\tilde{B}_1 - \tilde{B}_2$ is a linear combination of continuous Gaussian mean 0 processes, so it is a continuous Gaussian mean 0 process, and is thus characterized by its covariance. Since $\tilde{B}_1(\cdot)$ and $\tilde B_2(\cdot)$ are both Gaussian with mean 0 and the same covariance, their difference $\tilde{B}_1(\cdot) - \tilde{B}_2(\cdot)$ is also Gaussian with the same mean and covariance. This implies that $\tilde{B_1}-\tilde{B_2}$ is itself a Brownian bridge $\tilde B$ on $[a,b]$, and hence equation \ref{BBdifeq} can be rewritten  \begin{equation*}
		B_1(t)-B_2(t)=(b-a)^{1/2}\cdot\tilde{B}\left(\frac{t-a}{b-a}\right)+\left(\frac{b-t}{b-a}\right)\cdot (x_1-x_2)+\left(\frac{t-a}{b-a}\right)\cdot (y_1-y_2).
	\end{equation*}
	This is a Brownian bridge on $[a,b]$ from $x_1-x_2$ to $y_1-y_2$. 
\end{proof}

To conclude this section, we prove Lemma \ref{inftydistinct}.

\begin{proof}
	Without loss of generality we may assume that $\mathcal{L}^N$ is the weak limit of $(f^N - \lambda s^2)/\sqrt{p(1-p)}$ as $N\to\infty$. By the Skorohod representation theorem, there is a probability space $(\Omega,\mathcal{F},\mathbb{P})$ supporting random variables $\mathcal{X}^N$ and $\mathcal{X}$ with the laws of $f^N$ and $f^\infty$ respectively, such that $\mathcal{X}^N \to \mathcal{X}$ uniformly on compact sets as $N\to\infty$, pointwise on all of $\Omega$. In particular, $\mathcal{X}^N(s) \to \mathcal{X}(s)$. We have $f^N_i(s) = N^{-\alpha/2}(L^N_i(sN^\alpha) - psN^\alpha) + \lambda s^2$, so $\mathcal{X}^N_i(s) = N^{-\alpha/2}(\mathcal{L}^N_i(sN^\alpha) - psN^\alpha)/\sqrt{p(1-p)}$, where $\mathcal{L}^N$ has the law of $L^N$.
	
	Suppose that $\mathcal{X}_i(s) = \mathcal{X}_{i+1}(s)$ for some $i\in\llbracket 1,k-2\rrbracket$. Then we have $\mathcal{X}_i^N(s) - \mathcal{X}^N_{i+1}(s) \to 0$, i.e., $N^{-\alpha/2}(\mathcal{L}_i^N(sN^\alpha) - \mathcal{L}^N_{i+1}(sN^\alpha))\to 0$ as $N\to\infty$. Let us write $a = \lfloor sN^\alpha\rfloor N^{-\alpha}$, $b = \lceil (s+2)N^\alpha\rceil N^{-\alpha}$ and $x^N = \mathcal{L}_i^N(aN^\alpha) - \mathcal{L}_{i+1}^N(aN^\alpha)$, $y^N = \mathcal{L}_i^N(bN^\alpha) - \mathcal{L}_{i+1}^N(bN^\alpha)$. Then $N^{-\alpha/2}x^N\to 0$. If $Q_i,Q_{i+1}$ are independent Bernoulli bridges with laws $\mathbb{P}^{a,b,\mathcal{L}_i^N(aN^\alpha),\mathcal{L}_i^N(bN^\alpha)}_{Ber}$ and $\mathbb{P}^{a,b,\mathcal{L}_{i+1}^N(aN^\alpha),\mathcal{L}_{i+1}^N(bN^\alpha)}_{Ber}$, then $\ell=Q_i - Q_{i+1}$ is a random walk bridge taking values in $\{-1,0,1\}$, from $(a,x^N)$ to $(b,y^N)$. Let us denote the law of $N^{-\alpha/2}\ell/\sqrt{p(1-p)}$ by $\mathbb{P}^{a,b,x^N,y^N}_{di\!f\!f}$.
	
	By Lemma \ref{scaledRWbb}, $(x^N+N^{-\alpha/2}Q_{i+1}-ptN^\alpha)/\sqrt{p(1-p)}$ and $(x^N+N^{-\alpha/2}Q_i-ptN^\alpha)/\sqrt{p(1-p)}$ converge weakly to the law of two Brownian bridges $B^1$ and $B^2$ respectively, and hence their difference $N^{-\alpha/2}\ell/\sqrt{p(1-p)}$ converges weakly to the difference of two independent Brownian bridges, $B^1-B^2$. 
	By Lemma \ref{BBDif}, this difference is itself a Brownian bridge $B$ on $[s,s+2]$ from 0 to $y$, i.e., $B$ has law $\mathbb{P}^{s,s+2,0,y}_{free}$. Therefore $\mathbb{P}^{a,b,x^N,y^N}_{dif\!f}$ converges weakly to $\mathbb{P}^{s,s+2,0,y}_{free}$. With probability one, $\min_{t\in[s,s+2]} B_t < 0$ by Lemma \ref{BBcross0}. Thus given $\delta > 0$, we can choose $N$ large enough so that the probability of $N^{-\alpha/2}\ell/\sqrt{p(1-p)}$, or equivalently $\ell$, remaining above 0 on $[a,b]$ is less than $\delta$. Thus for large enough $N$ we have
	\begin{equation}\label{collideAP}
		\begin{split}
		&\mathbb{P}\left(f_i^\infty(s) = f_{i+1}^\infty(s)\right) \leq \mathbb{P}\bigg(\mathbb{P}^{a,b,x^N,y^N}_{dif\!f}\bigg(\sup_{s\in[a,b]}\ell(s) \geq 0\bigg) < \delta\bigg) \leq\\
		& \mathbb{P}\left(Z(a,b,\mathcal{L}^N(aN^\alpha),\mathcal{L}^N(bN^\alpha),\infty,\mathcal{L}^N_k) < \delta\right).
		\end{split}
	\end{equation}
	Here, $Z$ denotes the acceptance probability of Definition \ref{DefAP}. This is the probability that $k-1$ independent Bernoulli bridges $Q_1,\dots,Q_{k-1}$ on $[a,b]$ with entrance and exit data $\mathcal{L}^N(a)$ and $\mathcal{L}^N(b)$ do not cross one another or $\mathcal{L}_k^N$. The last inequality follows because $\ell$ has the law of the difference of $Q_i$ and $Q_{i+1}$, and the acceptance probability is bounded above by the probability that $Q_i$ and $Q_{i+1}$ do not cross, i.e., that $Q_i - Q_{i+1} \geq 0$. By Proposition \ref{PropMain}, given $\epsilon > 0$ we can choose $\delta$ so that the probability on the right in \eqref{collideAP} is $<\epsilon$. We conclude that
	\[
	\mathbb{P}\left(f_i^\infty(s) = f_{i+1}^\infty(s)\right) = 0.
	\]
\end{proof}



\subsection{Proof of Lemmas \ref{MCLxy} and \ref{MCLfg}}

We will prove the following lemma, of which the two lemmas are immediate consequences. In particular, Lemma \ref{MCLxy} is the special case when $g^b = g^t$, and Lemma \ref{MCLfg} is the case when $\vec{x} = \vec{x}\,'$ and $\vec{y} = \vec{y}\,'$. We argue in analogy to \cite[Lemma 5.6]{DimMat}.

\begin{lemma}
	Fix $k \in \mathbb{N}$, $T_0, T_1 \in \mathbb{Z}$ with $T_0 < T_1$, $S\subseteq\llbracket T_0, T_1\rrbracket$, and two functions $g^b, g^t: \llbracket T_0, T_1 \rrbracket  \rightarrow [-\infty, \infty)$ with $g^b\leq g^t$ on $S$. Also fix $\vec{x}, \vec{y}, \vec{x}\,', \vec{y}\,' \in \mathfrak{W}_k$ such that $x_i\leq x_i'$, $y_i\leq y_i'$ for $1\leq i\leq k$. Assume that $\Omega_{avoid}(T_0, T_1, \vec{x}, \vec{y}, \infty,g^b; S)$ and $\Omega_{avoid}(T_0, T_1, \vec{x}\,', \vec{y}\,', \infty,g^t; S)$ are both non-empty. Then there exists a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, which supports two $\llbracket 1, k \rrbracket$-indexed Bernoulli line ensembles $\mathfrak{L}^t$ and $\mathfrak{L}^b$ on $\llbracket T_0, T_1 \rrbracket$ such that the law of $\mathfrak{L}^{t}$ {\big (}resp. $\mathfrak{L}^b${\big )} under $\mathbb{P}$ is given by $\mathbb{P}_{avoid, Ber; S}^{T_0, T_1, \vec{x}\,', \vec{y}\,', \infty, g^t}$ {\big (}resp. $\mathbb{P}_{avoid, Ber; S}^{T_0, T_1, \vec{x}, \vec{y}, \infty, g^b}${\big )} and such that $\mathbb{P}$-almost surely we have $\mathfrak{L}_i^t(r) \geq \mathfrak{L}^b_i(r)$ for all $i = 1,\dots, k$ and $r \in \llbracket T_0, T_1 \rrbracket$.
\end{lemma}

\begin{proof} Throughout the proof, we will write $\Omega_{a,S}$ to mean $\Omega_{avoid}(T_0,T_1,\vec{x},\vec{y},\infty,g^b;S)$ and $\Omega_{a,S}'$ to mean $\Omega_{avoid}(T_0,T_1,\vec{x}\,',\vec{y}\,',\infty,g^t;S)$. We split the proof into two steps.\\
	
	\noindent\textbf{Step 1.} We first aim to construct a Markov chain $(X^n,Y^n)_{n\geq 0}$, with $X^n\in \Omega_{a,S}$, $Y^n\in \Omega_{a,S}'$, with initial distribution given by
	\begin{align*}
	& X^0_i(t) = \min(x_i+t-T_0, \, y_i), \qquad Y^0_i(t) = \min(x_i'+t-T_0, \, y_i'),
	\end{align*}
	for $t\in\llbracket T_0, T_1\rrbracket$ and $1\leq i\leq k$. First observe that we do in fact have $X^0 \in \Omega_{a,S}$, since $X_i^0(T_0) = x_i$, $X_i^0(T_1) = y_i$, $X_i^0(t) \leq \min(x_{i-1} + t - T_0, \, y_{i-1}) = X_{i-1}^0(t)$, and $X_k^0(t) \geq x_i + t - T_0 \geq g^b(T_0) + t - T_0 \geq g^b(t)$. We also note here that $X^0$ is \textit{maximal} on the entire space $\Omega(T_0,T_1,\vec{x},\vec{y})$, in the sense that for any $Z\in\Omega(T_0,T_1,\vec{x},\vec{y})$, we have $Z_i(t) \leq X^0_i(t)$ for all $t\in\llbracket T_0, T_1 \rrbracket$. In particular, $X^0$ is maximal on $\Omega_{a,S}$. Likewise, we see that $Y^0$ is maximal on $\Omega'_{a,S}$.
	
	We want the chain $(X^n,Y^n)$ to have the following properties: 
	\begin{enumerate}[label=(\arabic*)]
		
		\item $(X^n)_{n\geq 0}$ and $(Y^n)_{n\geq 0}$ are both Markov in their own filtrations,
		
		\item $(X^n)$ is irreducible and aperiodic, with invariant distribution $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x},\vec{y},\infty,g^b}$,
		
		\item $(Y^n)$ is irreducible and aperiodic, with invariant distribution $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x}\,',\vec{y}\,',\infty,g^t}$,
		
		\item $X^n_i\leq Y^n_i$ on $\llbracket T_0, T_1 \rrbracket$ for all $n\geq 0$ and $1\leq i \leq k$.
		
	\end{enumerate}
	
	\noindent This will allow us to conclude convergence of $X^n$ and $Y^n$ to these two uniform measures.
	
	We specify the dynamics of $(X^n, Y^n)$ as follows. At time $n$, we uniformly sample a triple $(i,t,z)\in \llbracket 1, k\rrbracket \times \llbracket T_0, T_1\rrbracket \times \llbracket x_k,y_1'-1\rrbracket$. We also flip a fair coin, with $\mathbb{P}(\textrm{heads})=\mathbb{P}(\textrm{tails})=1/2$. We update $X^n$ and $Y^n$ using the following procedure. If $j\neq i$, we leave $X_j,Y_j$ unchanged, and for all points $s\neq t$, we set $X^{n+1}_i(s) = X^n_i(s)$. If $T_0 < t < T_1$, $X^n_i(t-1)=z$, and $X^n_i(t+1)=z+1$ (note that this implies $X^n_i(t)\in\{z,z+1\}$), we consider two cases. If $t \in S$, then we set
	\[
	X^{n+1}_i(t) = \begin{cases}
	z+1, & \textrm{if heads},\\
	z, & \textrm{if tails},
	\end{cases}
	\]
	assuming this does not cause $X^{n+1}_i(t)$ to fall below $X^n_{i+1}(t)$, with the convention that $X^n_{k+1} = g^b$. If $t\notin S$, we perform the same update regardless of whether it results in a crossing. In all other cases, we leave $X^{n+1}_i(t)=X^n_i(t)$. We update $Y^n$ using the same rule, with $g^t$ in place of $g^b$.
	
	We first observe that $X^n$ and $Y^n$ are in fact non-intersecting on $S$ for all $n$. Note $X^0$ is non-intersecting, and if $X^n$ is non-intersecting, then the only way $X^{n+1}$ could be intersecting on $S$ is if the update were to push $X^{n+1}_i(t)$ below $X^n_{i+1}(t)$ for some $i,t$ with $t\in S$. But any update of this form is suppressed, so it follows by induction that $X^n\in\Omega_{a,S}$ for all $n$. Similarly, we see that $Y^n\in\Omega_{a,S}'$.
	
	It is easy to see that $(X^n,Y^n)$ is a Markov chain, since at each time $n$, the value of $(X^{n+1},Y^{n+1})$ depends only on the current state $(X^n,Y^n)$, and not on the time $n$ or any of the states prior to time $n$. Moreover, the value of $X^{n+1}$ depends only on the state $X^n$, not on $Y^n$, so $(X^n)$ is a Markov chain in its own filtration. The same applies to $(Y^n)$. This proves the property (1) above.
	
	We now argue that $(X^n)$ and $(Y^n)$ are irreducible. Fix any $Z \in \Omega_{a;S}$. As observed above, we have $Z_i \leq X^0_i$ on $\llbracket T_0,T_1\rrbracket$ for all $i$. We argue that we can reach the state $Z$ starting from $X^0$ in some finite number of steps with positive probability. Due to the maximality of $X^0$, we only need to move the paths downward. If we do this starting with the bottom path, then there is no danger of the paths $X_i$ crossing on $S$, or of $X_k$ crossing $g^b$ on $S$. To ensure that $X^n_k = Z_k$, we successively sample triples $(k,t,z)$ as follows. We initialize $t = T_0 + 1$. If $X_k^n(t) = Z_k(t)$, we increment $t$ by 1. Otherwise, we have $X_k^n(t) > Z_k(t)$, so we set $z = X_k^n(t) - 1$ and flip tails. This may or may not push $X_k(t)$ downwards by 1. We then increment $t$ and repeat this process. If $t$ reaches $T_1 - 1$, then at the increment we reset $t = T_0 + 1$. After finitely many steps, $X_k$ will agree with $Z_k$ on all of $\llbracket T_0, T_1 \rrbracket$. We then repeat this process for $X^n_i$ and $Z_i$, with $i$ descending. Since each of these samples and flips has positive probability, and this process terminates in finitely many steps, the probability of transitioning from $X^n$ to $Z$ after some number of steps is positive. The same reasoning applies to show that $(Y^n)$ is irreducible.
	
	To see that the chains are aperiodic, simply observe that if we sample a triple $(i,T_0,z)$ or $(i,T_1,z)$, then the states of both chains will be unchanged.
	
	To see that the uniform measure $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x},\vec{y},\infty,g^b}$ on $\Omega_{a,S}$ is invariant for $(X^n)$, fix any $\omega\in\Omega_{a,S}$. For simplicity, write $\mu$ for the uniform measure. Then for all $\tau\in\Omega_{a,S}$, we have $\mu(\tau) = 1/|\Omega_{a,S}|$. Hence
	\begin{align*}
	& \sum_{\tau\in\Omega_{a,S}} \mu(\tau)\mathbb{P}(X^{n+1} = \omega\,|\,X^n = \tau) = \frac{1}{|\Omega_{a,S}|}\sum_{\tau\in\Omega_{a,S}} \mathbb{P}(X^{n+1} = \omega\,|\,X^n = \tau) = \\
	& \frac{1}{|\Omega_{a,S}|}\sum_{\tau\in\Omega_{a,S}} \mathbb{P}(X^{n+1} = \tau\,|\,X^n = \omega) = \frac{1}{|\Omega_{a,S}|}\cdot 1 = \mu(\omega).
	\end{align*}
	The second equality is clear if $\tau=\omega$. Otherwise, note that $\mathbb{P}(X_{n+1} = \omega\,|\,X_n = \tau) \neq 0$ if and only if $\tau$ and $\omega$ differ only in one indexed path (say the $i$th) at one point $t$, where $|\tau_i(t)-\omega_i(t)|=1$, and this condition is also equivalent to $\mathbb{P}(X^{n+1} = \tau\,|\,X^n = \omega) \neq 0$. If $X^n=\tau$, there is exactly one choice of triple $(i,t,z)$ and one coin flip which will ensure $X^{n+1}_i(t)=\omega(t)$, i.e., $X^{n+1}=\omega$. Conversely, if $X^n=\omega$, there is one triple and one coin flip which will ensure $X^{n+1}=\tau$. Since the triples are sampled uniformly and the coin flips are fair, these two conditional probabilities are in fact equal. This proves (2), and an analogous argument proves (3).
	
	Lastly, we argue that $X^n_i\leq Y^n_i$ on $\llbracket T_0, T_1 \rrbracket$ for all $n\geq 0$ and $1\leq i\leq k$. This is of course true at $n=0$. Suppose it holds at some $n\geq 0$, and suppose that we sample a triple $(i,t,z)$. Then the update rule can only change the values of the $X_i^n(t)$ and $Y_i^n(t)$. Notice that the values can change by at most 1, and if $Y^n_i(t) - X^n_i(t) = 1$, then the only way the ordering could be violated is if $Y_i$ were lowered and $X_i$ were raised at the next update. But this is impossible, since a coin flip of heads can only raise or leave fixed both curves, and tails can only lower or leave fixed both curves. Thus it suffices to assume $X^n_i(t) = Y^n_i(t)$.
	
	There are two cases to consider that violate the ordering of $X^{n+1}_i(t)$ and $Y^{n+1}_i(t)$. Either (i) $X_i(t)$ is raised but $Y_i(t)$ is left fixed, or (ii) $Y_i(t)$ is lowered yet $X_i(t)$ is left fixed. These can only occur if the curves exhibit one of two specific shapes on $\llbracket t-1, t+1\rrbracket$. For $X_i(t)$ to be raised, we must have $X^n_i(t-1) = X^n_i(t) = X^n_i(t+1) - 1$, and for $Y_i(t)$ to be lowered, we must have $Y^n_i(t-1) - 1 = Y^n_i(t) = Y^n_i(t+1)$. From the assumptions that $X^n_i(t) = Y^n_i(t)$, and $X^n_i \leq Y^n_i$, we observe that both of these requirements force the other curve to exhibit the same shape on $\llbracket t-1, t+1\rrbracket$. Then the update rule will be the same for both curves for either coin flip, proving that both (i) and (ii) are impossible. \\
	
	\noindent\textbf{Step 2.} It follows from (2) and (3) and \cite[Theorem 1.8.3]{Norris} that $(X^n)_{n\geq 0}$ and $(Y^n)_{n\geq 0}$ converge weakly to $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x},\vec{y},\infty,g^b}$ and $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x}\,',\vec{y}\,',\infty,g^t}$ respectively. In particular, $(X^n)$ and $(Y^n)$ are tight, so $(X^n,Y^n)_{n\geq 0}$ is tight as well. By Prohorov's theorem, it follows that $(X^n,Y^n)$ is relatively compact. Let $(n_m)$ be a sequence such that $(X^{n_m},Y^{n_m})$ converges weakly. Then by the Skorohod representation theorem \cite[Theorem 6.7]{Billing}, it follows that there exists a probability space $(\Omega,\mathcal{F},\mathbb{P})$ supporting random variables $\mathfrak{X}^n$, $\mathfrak{Y}^n$ and $\mathfrak{X},\mathfrak{Y}$ taking values in $\Omega_{a,S},\Omega_{a,S}'$ respectively, such that
	\begin{enumerate}[label=(\arabic*)]
		
		\item The law of $(\mathfrak{X}^n,\mathfrak{Y}^n)$ under $\mathbb{P}$ is the same as that of $(X^n,Y^n)$,
		
		\item $\mathfrak{X}^n(\omega) \longrightarrow \mathfrak{X}(\omega)$ for all $\omega\in\Omega$,
		
		\item $\mathfrak{Y}^n(\omega) \longrightarrow \mathfrak{Y}(\omega)$ for all $\omega\in\Omega$.
		
	\end{enumerate}
	
	In particular, (1) implies that $\mathfrak{X}^{n_m}$ has the same law as $X^{n_m}$, which converges weakly to $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x},\vec{y},\infty,g^b}$. It follows from (2) and the uniqueness of limits that $\mathfrak{X}$ has law $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x},\vec{y},\infty,g^b}$. Similarly, $\mathfrak{Y}$ has law $\mathbb{P}_{avoid,Ber;S}^{T_0,T_1,\vec{x}\,',\vec{y}\,',\infty,g^t}$. Moreover, condition (4) in Step 1 implies that $\mathfrak{X}^n_i \leq \mathfrak{Y}^n_i$, $\mathbb{P}$-a.s., so $\mathfrak{X}_i \leq \mathfrak{Y}_i$ for $1\leq i\leq k$, $\mathbb{P}$-a.s. Thus we can take $\mathfrak{L}^b = \mathfrak{X}$ and $\mathfrak{L}^t = \mathfrak{Y}$.
	
\end{proof}
